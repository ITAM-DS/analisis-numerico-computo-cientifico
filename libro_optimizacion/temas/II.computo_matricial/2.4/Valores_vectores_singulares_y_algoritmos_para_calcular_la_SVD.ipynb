{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(VALVECSINGALGSVD)="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Valores, vectores singulares y algoritmos para calcular la SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Notas para contenedor de docker:\n",
    "\n",
    "Comando de docker para ejecución de la nota de forma local:\n",
    "\n",
    "nota: cambiar `<ruta a mi directorio>` por la ruta de directorio que se desea mapear a `/datos` dentro del contenedor de docker.\n",
    "\n",
    "`docker run --rm -v <ruta a mi directorio>:/datos --name jupyterlab_optimizacion -p 8888:8888 -d palmoreck/jupyterlab_optimizacion:2.1.4`\n",
    "\n",
    "password para jupyterlab: `qwerty`\n",
    "\n",
    "Detener el contenedor de docker:\n",
    "\n",
    "`docker stop jupyterlab_optimizacion`\n",
    "\n",
    "Documentación de la imagen de docker `palmoreck/jupyterlab_optimizacion:2.1.4` en [liga](https://github.com/palmoreck/dockerfiles/tree/master/jupyterlab/optimizacion).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota generada a partir de [liga](https://www.dropbox.com/s/s4ch0ww1687pl76/3.2.2.Factorizaciones_matriciales_SVD_Cholesky_QR.pdf?dl=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Al final de esta nota el y la lectora:\n",
    ":class: tip\n",
    "\n",
    "* Aprenderá algunas definiciones y resultados de los valores y vectores singulares.\n",
    "\n",
    "* Se proporcionará una lista de algoritmos para calcular la descomposición en valores singulares.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta nota **asumimos** que $A \\in \\mathbb{R}^{m \\times n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor singular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "El número $\\sigma$ se denomina valor *singular* de $A$ si $\\sigma = \\sqrt{\\lambda_{A^TA}} = \\sqrt{\\lambda_{AA^T}}$ donde: $\\lambda_{A^TA}$ y $\\lambda_{AA^T}$ es eigenvalor de $A^TA$ y $AA^T$ respectivamente.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "La definición se realiza sobre $A^TA$ o $AA^T$ pues éstas matrices tienen el mismo espectro y además sus eigenvalores son reales y no negativos por lo que $\\sigma \\in \\mathbb{R}$ y de hecho $\\sigma \\geq 0$ (la raíz cuadrada se calcula para un eigenvalor no negativo).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector singular izquierdo, vector singular derecho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "Asociado con cada valor singular $\\sigma$ existen vectores singulares $u,v$ que cumplen con la igualdad: \n",
    "\n",
    "$$Av = \\sigma u .$$ \n",
    "\n",
    "Al vector $u$ se le nombra **vector singular izquierdo** y al vector $v$ se le nombra **vector singular derecho**.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descomposición en valores singulares (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si $A \\in \\mathbb{R}^{mxn}$ entonces existen $U \\in \\mathbb{R}^{mxm}, V \\in \\mathbb{R}^{nxn}$ **ortogonales** tales que: $A = U\\Sigma V^T$ con $\\Sigma = diag(\\sigma_1, \\sigma_2, \\dots, \\sigma_p) \\in \\mathbb{R}^{mxn}$, $p = \\min\\{m,n\\}$ y $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_p \\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo para un caso $m < n$:\n",
    "\n",
    "$$\n",
    "\\left [\n",
    "\\begin{array}{ccc}\n",
    "a_{11} & a_{12} & a_{13}\\\\\n",
    "a_{21} & a_{22} & a_{23}\n",
    "\\end{array}\n",
    "\\right ]\n",
    "=\n",
    "\\left [\n",
    "\\begin{array}{cc}\n",
    "u_{11} & u_{12}\\\\\n",
    "u_{22} & u_{22}\n",
    "\\end{array}\n",
    "\\right ]\n",
    "\\left [\n",
    "\\begin{array}{ccc}\n",
    "\\sigma_1 & 0 & 0\\\\\n",
    "0 & \\sigma_2 & 0\n",
    "\\end{array}\n",
    "\\right ]\n",
    "\\left [\n",
    "\\begin{array}{ccc}\n",
    "v_{11} & v_{21} & v_{31}\\\\\n",
    "v_{12} & v_{22} & v_{32}\\\\\n",
    "v_{13} & v_{23} & v_{33}\n",
    "\\end{array}\n",
    "\\right ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y para un caso $m > n$:\n",
    "\n",
    "$$\n",
    "\\left [\n",
    "\\begin{array}{cc}\n",
    "a_{11} & a_{12}\\\\\n",
    "a_{21} & a_{22}\\\\\n",
    "a_{31} & a_{32}\n",
    "\\end{array}\n",
    "\\right ]\n",
    "=\n",
    "\\left [\n",
    "\\begin{array}{ccc}\n",
    "u_{11} & u_{12} & u_{13}\\\\\n",
    "u_{22} & u_{22} & u_{23}\\\\\n",
    "u_{31} & u_{32} & u_{33}\n",
    "\\end{array}\n",
    "\\right ]\n",
    "\\left [\n",
    "\\begin{array}{cc}\n",
    "\\sigma_1 & 0\\\\\n",
    "0 & \\sigma_2\\\\\n",
    "0 & 0\n",
    "\\end{array}\n",
    "\\right ]\n",
    "\\left [\n",
    "\\begin{array}{cc}\n",
    "v_{11} & v_{21}\\\\\n",
    "v_{12} & v_{22}\\\\\n",
    "\\end{array}\n",
    "\\right ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "Las columnas de $U$ nombramos **vectores singulares izquierdos de $A$** y las columnas de $V$ nombramos **vectores singulares derechos de $A$** en $A = U \\Sigma V^T$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* La notación $\\sigma_1$ hace referencia al valor singular más grande de A, $\\sigma_2$ al segundo valor singular más grande de A y así sucesivamente.\n",
    "\n",
    "* Para cualquier $A \\in \\mathbb{R}^{m \\times n}$ se tiene $A = U \\Sigma V^T$. Si $b = Ax = (U \\Sigma V^T) x$ entonces:\n",
    "\n",
    "$$\\tilde{b} = U^Tb = U^T  (Ax) = U^T (U \\Sigma V^T) x = \\Sigma V^Tx = \\Sigma \\tilde{x}.$$\n",
    "\n",
    "Lo anterior indica que el producto matricial $Ax$ para cualquier matriz $A$ es equivalente a multiplicar una matriz diagonal por un vector denotado como $\\tilde{x}$ que contiene los coeficientes de la combinación lineal de las columnas de $V$ para el vector $x$ . El resultado de tal multiplicación es un vector denotado como $\\tilde{b}$ que contiene los coeficientes de la combinación lineal de las columnas de $U$ para el vector $b$. En resúmen, la multiplicación $Ax$ es equivalente a la multiplicación por una matriz diagonal $\\Sigma \\tilde{x}$ salvo dos cambios de bases, ver [Change of basis](https://en.wikipedia.org/wiki/Change_of_basis), la base de los vectores singulares derechos (columnas de $V$) y la base de los vectores singulares izquierdos (columnas de $U$).\n",
    "\n",
    "\n",
    "* La SVD que se definió arriba es nombrada *SVD full*, hay otras formas como la **truncada** en la que $U \\in \\mathbb{R}^{m \\times k}$, $V \\in \\mathbb{R}^{nxk}$ y $\\Sigma \\in \\mathbb{R}^{k \\times k}$ con $k \\leq r$, **compacta** donde $k=r$ y la *thin* en la que $k=p$:\n",
    "\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/8dq0jiw5em93k1j/svd_thin.png?dl=0\" heigth=\"700\" width=\"700\">\n",
    "\n",
    "\n",
    "donde: $r = rank(A)$.\n",
    "\n",
    "Ver [reduced SVDs](https://en.wikipedia.org/wiki/Singular_value_decomposition#Reduced_SVDs).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algunas propiedades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen diferentes propiedades de los valores y vectores singulares, aquí se enlistan algunas:\n",
    "\n",
    "* Si $rank(A) = r$ entonces $r \\leq p$ y $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r > \\sigma_{r+1} = \\sigma_{r+2} = \\dots = \\sigma_p =  0$.\n",
    "\n",
    "* Si $rank(A) = r$ entonces $A = \\displaystyle \\sum_{i=0}^r \\sigma_i u_i v_i^T$ con $u_i$ $i$-ésima columna de U y $v_i$ $i$-ésima columna de V.\n",
    "\n",
    "* Geométricamente los valores singulares de una matriz $A \\in \\mathbb{R}^{mxn}$ son las longitudes de los semiejes del hiperelipsoide $E$ definido por $E = \\{Ax : ||x||_2 \\leq 1\\}$ y los vectores $u_i$ son direcciones de estos semiejes; los vectores $vi$'s tienen norma igual a $1$ por lo que se encuentran en una circunferencia de radio igual a $1$ y como $Av_i = \\sigma u_i$ entonces $A$ mapea los vectores $v_i$'s a los semiejes $u_i$'s respectivamente:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/1yqoe4qibyyej53/svd_2.jpg?dl=0\" heigth=\"700\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La SVD da bases ortogonales para los $4$ espacios fundamentales de una matriz: espacio columna, espacio nulo izquierdo, espacio nulo y espacio renglón:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/uo9s9f0nqi43s6d/svd_four_spaces_of_matrix.png?dl=0\" heigth=\"600\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Si $t < r$ y $r=rank(A)$ entonces $A_t =  \\displaystyle \\sum_{i=0}^t \\sigma_i u_i v_i^T$ (SVD truncada) es una matriz de entre todas las matrices con $rank$ igual a t, que es más *cercana* a A. La cercanía se mide con la norma **matricial** Euclidiana y de Frobenius, en el caso de Frobenius es la única matriz que cumple lo anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algunas aplicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas de las aplicaciones de la SVD se encuentran:\n",
    "\n",
    "* Procesamiento de imágenes y señales.\n",
    "* Sistemas de recomendación (Netflix).\n",
    "* Mínimos cuadrados.\n",
    "* Componentes principales.\n",
    "* Reconstrucción de imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos numéricos para calcular SVD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin} \n",
    "\n",
    "En *NumPy* con [numpy.linalg.svd](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html) podemos calcular la SVD de $A$, obsérvese en la ayuda  que se regresa $V^T$ y no $V$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Algunos métodos para calcular la descomposición en valores singulares de una matriz son:\n",
    "\n",
    "* Método de rotaciones de Jacobi ***one sided***, ver {ref}`rotaciones de Jacobi para matrices simétricas <ROTJACMATSIM>` en el que se utiliza el ***two sided***.\n",
    "\n",
    "* [Bidiagonalización](https://en.wikipedia.org/wiki/Bidiagonalization). \n",
    "\n",
    "* [Método de la potencia](https://en.wikipedia.org/wiki/Power_iteration) en el que se utiliza el **[cociente de Rayleigh](https://en.wikipedia.org/wiki/Rayleigh_quotient)** para acelerar convergencia, ver {ref}`método de la potencia para matrices simétricas <MPOTMATSIM>` y la {ref}`iteración por el cociente de Rayleigh para matrices simétricas <ITERCRAYMATSIM>`.\n",
    "\n",
    "* [Algoritmo QR](https://en.wikipedia.org/wiki/QR_algorithm) que se basa en la factorización QR, ver {ref}`algoritmo QR o QR iteration (versión simple) para matrices simétricas <ALGQR>`.\n",
    "\n",
    "* Métodos de descenso aplicados a problemas de optimización. \n",
    "\n",
    "* Para casos particulares como una matriz $A$ *sparse* o rala (gran cantidad de ceros) se utilizan algoritmos como [**Lanczos Golub Kahan bidiagonalization**](http://www.netlib.org/utk/people/JackDongarra/etemplates/node198.html) que forma parte de una amplia clases de métodos nombrados [**Krylov subspace methods**](https://en.wikipedia.org/wiki/Krylov_subspace) y el algoritmo de [**tridiagonalización Lanczos**](https://en.wikipedia.org/wiki/Lanczos_algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método de rotaciones de Jacobi *one sided*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este método se utilizan rotaciones Givens, ver {ref}`transformaciones de rotación <TROT>`, para construir a la matriz ortogonal $V \\in \\mathbb{R}^{n \\times n}$ y llegar a una matriz $W$: \n",
    "\n",
    "$$AV \\rightarrow W \\in  \\mathbb{R}^{m \\times n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentario\n",
    "\n",
    "Las normas Euclidianas de las columnas de $W$ construyen a los valores singulares $\\sigma_i \\forall i=1,\\dots,r$:\n",
    "\n",
    "$$W = [U_1 \\quad 0]\\left[ \\begin{array}{cc}\n",
    "\\Sigma & 0\\\\\n",
    "0 & 0\n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "con $U_1 \\in \\mathbb{R}^{m \\times r}$ matriz con columnas ortonormales: $U_1^TU_1=I_r$ y $\\Sigma = diag(\\sigma_1,\\dots, \\sigma_r)$ matriz diagonal.\n",
    "\n",
    "\n",
    "Esta SVD es una forma compacta.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo: Método de rotaciones de Jacobi *one sided*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se denota $A_k=[a_1^{(k)} a_2^{(k)} \\cdots a_n^{(k)}]$ con cada $a_i^{(k)}$ como $i$-ésima columna de $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dados** $A \\in \\mathbb{R}^{m \\times n}$ y $tol >0$ **definir** $A_0 = A$, $Q_0 = I_n$.\n",
    ">\n",
    "> **Repetir** el siguiente bloque para $k=0,1,2,\\dots$\n",
    ">> 1. Elegir un par de índices $(idx1,idx2)$ con alguna de las metodologías descritas en el bloque siguiente de comentarios.\n",
    ">>\n",
    ">> 2. Revisar si las columnas $a_i^{(k)}, a_j^{(k)}$ de $A^{(k)}$ son ortogonales (el chequeo se describe en los comentarios). Si son ortogonales se incrementa por uno la variable $num\\text{_}columnas\\text{_}ortogonales$. Si no son ortogonales: Calcular $\\left[ \\begin{array}{cc} a & c\\\\ c & b \\end{array} \\right]$ la submatriz $(i,j)$ de $A_k^{T}A_k$ donde: $a = ||a_i^{(k)}||_2^2, b=||a_j^{(k)}||_2^2, c=a_i^{T(k)}a_j^{(k)}$. \n",
    ">>\n",
    ">> 3. Si no fueron ortogonales las columnas del paso 2, calcular las entradas $c: = \\cos(\\theta), s:=\\sin(\\theta)$ de la matriz de rotación $J_k$ que diagonaliza $\\left[ \\begin{array}{cc} a & c\\\\ c & b \\end{array} \\right]$.\n",
    ">>\n",
    ">> 4. Si no fueron ortogonales las columnas del paso 2, actualizar las columnas $i,j$ de $A_k$. \n",
    ">>\n",
    ">> 5. Si no fueron ortogonales las columnas del paso 2, actualizar a la matriz $V_k$.\n",
    ">\n",
    "> **hasta** convergencia: satisfacer criterio de paro en el que se utiliza $num\\text{_}columnas\\text{_}ortogonales$ y $maxsweeps$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Comentarios\n",
    "\n",
    "* En el método se hace mención de **metodologías** que ayudan a elegir los índices del renglón y columna del par de entradas de $A$ que serán eliminadas (hacer cercanas a cero). Algunas de éstas son:\n",
    "\n",
    "    * Elegir $(idx1,idx2)$ tales que $|a_{idx1,idx2}| = \\displaystyle \\max_{i \\neq j}|a_{ij}|$.\n",
    "\n",
    "    * **Ordenamiento cíclico por renglones:** elegir $(idx1, idx2)$ en el conjunto $(1,2),(1,3),\\dots,(1,n),(2,3),(2,4)\\dots,(n-1,n)$.\n",
    "\n",
    "* En matrices mayores a dos dimensiones el método de rotaciones de Jacobi *one sided* requiere **ortogonalización repetida** (volver a hacer columnas ortogonales) del par de columnas de $A$ seleccionadas de iteraciones previas pues en cada iteración vuelven a ser no ortogonales en general. \n",
    "\n",
    "     \n",
    "* ¿Cómo revisar si las columnas  $i,j$ de $A_k$ son ortogonales?  si se cumple que\n",
    "\n",
    "$$\\frac{|a_i^{T (k)}a_j^{(k)}|}{||a_i^{(k)}||_2||a_j^{(k)}||_2} < tol$$\n",
    "\n",
    "con $tol$ un valor menor o igual a $10^{-8}$ entonces son ortogonales las columnas $a_i^{(k)}, a_j^{(k)}$ de $A_k$.\n",
    "\n",
    "* Las entradas de la matriz $J_k$ son: $\\tau = \\frac{b-a}{2c}, t^*=\\frac{signo(\\tau)}{|\\tau| + \\sqrt{1+\\tau^2}}, c = \\frac{1}{\\sqrt{1+t^{*2}}}, s = ct^*$.   \n",
    "\n",
    "* Para actualizar las columnas $i,j$ de $A_k$ utilizar: para $\\ell$ de $1$ a $n$:\n",
    "    \n",
    "    * $temp = A^{(k)}_{\\ell i}$\n",
    "\n",
    "    * $A_{\\ell i}^{(k)} = c*temp - s*A_{\\ell j}^{(k)}$\n",
    "\n",
    "    * $A_{\\ell j}^{(k)} = s*temp + c*A_{\\ell j}^{(k)}$\n",
    "        \n",
    "* Para actualizar a la matriz $V_k$ utilizar: para $\\ell$ de $1$ a $n$:\n",
    "    \n",
    "    * $temp = V_{\\ell i}^{(k)}$\n",
    "\n",
    "    * $V_{\\ell i}^{(k)} = c*temp - s*V_{\\ell j}^{(k)}$\n",
    "\n",
    "    * $V_{\\ell j}^{(k)} = s*temp + c*V_{\\ell j}^{(k)}$\n",
    "    \n",
    "* El método de rotaciones de Jacobi para matrices simétricas utiliza como criterios de paro:\n",
    "\n",
    "    * La cantidad $num\\text{_}columnas\\text{_}ortogonales$.\n",
    "    \n",
    "    * Número máximo de *sweeps*. Un *sweep* consiste de como máximo $\\frac{n(n-1)}{2}$ rotaciones (pues depende de cuántas columnas son o no ortogonales) y en cada *sweep* se ortogonalizan $2$ columnas. El criterio de paro es de la forma:\n",
    "\n",
    "```\n",
    "while num_columnas_ortogonales != n(n-1)/2 && sweeps < max_sweeps\n",
    "```\n",
    "\n",
    "con `sweeps` contador de los *sweeps*.\n",
    "    \n",
    "\n",
    "* Al finalizar el método, los valores singulares calculados son las normas Euclidianas de cada columna de $A_k$ y las columnas normalizadas de $A_k$ son las columnas de $U$. \n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas de comprehensión**\n",
    "\n",
    "1)¿Cómo se podría calcular el rank de una matriz si se han calculado previamente sus valores singulares?\n",
    "\n",
    "2)Verdadero o falso:\n",
    "\n",
    "a.Las columnas de la matriz V en la SVD de una matriz A, son eigenvectores de la matriz AA^T.\n",
    "\n",
    "b.Si el rank de una matriz es r, entonces las columnas r+1 a m de la matriz U en la SVD de la matriz A de tamaño mxn nos dan una base del espacio nulo izquierdo de A.\n",
    "\n",
    "c.La norma 2 de una matriz A es el mínimo valor singular de A.\n",
    "\n",
    "3)¿Cuál es la mejor aproximación a una matriz A bajo la norma de Frobenius que se puede obtener sobre el espacio de matrices de rank igual a t ?\n",
    "\n",
    "4)Menciona características y diferencias que tiene la *eigen decomposition* y la SVD de una matriz A (suponemos existe una *eigen decomposition*).\n",
    "\n",
    "5)Menciona métodos numéricos para calcular la SVD de una matriz.\n",
    "\n",
    "6)Menciona aplicaciones de la SVD de una matriz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias**\n",
    "\n",
    "1. L. Trefethen, D. Bau, Numerical linear algebra, SIAM, 1997.\n",
    "\n",
    "2.  G. H. Golub, C. F. Van Loan, Matrix Computations, John Hopkins University Press, 2013. \n",
    "\n",
    "3. C. Meyer, Matrix Analysis and Applied Linear Algebra, SIAM, 2000."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
