{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(DPOCFC)="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Definición de problemas de optimización, conjuntos y funciones convexas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Notas para contenedor de docker:\n",
    "\n",
    "Comando de docker para ejecución de la nota de forma local:\n",
    "\n",
    "nota: cambiar `<ruta a mi directorio>` por la ruta de directorio que se desea mapear a `/datos` dentro del contenedor de docker.\n",
    "\n",
    "`docker run --rm -v <ruta a mi directorio>:/datos --name jupyterlab_optimizacion -p 8888:8888 -d palmoreck/jupyterlab_optimizacion:2.1.4`\n",
    "\n",
    "password para jupyterlab: `qwerty`\n",
    "\n",
    "Detener el contenedor de docker:\n",
    "\n",
    "`docker stop jupyterlab_optimizacion`\n",
    "\n",
    "Documentación de la imagen de docker `palmoreck/jupyterlab_optimizacion:2.1.4` en [liga](https://github.com/palmoreck/dockerfiles/tree/master/jupyterlab/optimizacion).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota generada a partir de [liga1](https://www.dropbox.com/s/qb3swgkpaps7yba/4.1.Introduccion_optimizacion_convexa.pdf?dl=0), [liga2](https://www.dropbox.com/s/6isby5h1e5f2yzs/4.2.Problemas_de_optimizacion_convexa.pdf?dl=0), [liga3](https://www.dropbox.com/s/ko86cce1olbtsbk/4.3.1.Teoria_de_convexidad_Conjuntos_convexos.pdf?dl=0), [liga4](https://www.dropbox.com/s/mmd1uzvwhdwsyiu/4.3.2.Teoria_de_convexidad_Funciones_convexas.pdf?dl=0), [liga5](https://drive.google.com/file/d/1xtkxPCx05Xg4Dj7JZoQ-LusBDrtYUqOF/view), [liga6](https://drive.google.com/file/d/16-_PvWNaO0Zc9x04-SRsxCRdn5fxebf2/view)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Al final de esta nota el y la lectora:\n",
    ":class: tip\n",
    "\n",
    "* Conocerá la definición de un problema de optimización, algunos ejemplos, definiciones y resultados que serán utilizados en los métodos para resolver problemas de optimización con énfasis en funciones convexas.\n",
    "\n",
    "* Tendrá una lista ejemplo de funciones convexas utilizadas en aplicaciones.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Problemas de optimización numérica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una gran cantidad de aplicaciones plantean problemas de optimización. Tenemos problemas básicos que se presentan en cursos iniciales de cálculo:\n",
    "\n",
    "*Una caja con base y tapa cuadradas debe tener un volumen de $100 cm^3$. Encuentre las dimensiones de la caja que minimicen la cantidad de material.*\n",
    "\n",
    "Y tenemos más especializados que encontramos en áreas como estadística, ingeniería, finanzas o aprendizaje de máquina, *aka machine learning*:\n",
    "\n",
    "* Ajustar un modelo de regresión lineal a un conjunto de datos.\n",
    "\n",
    "* Buscar la mejor forma de invertir un capital en un conjunto de activos.\n",
    "\n",
    "* Elección del ancho y largo de un dispositivo en un circuito electrónico.\n",
    "\n",
    "* Ajustar un modelo que clasifique un conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general un problema de optimización matemática o numérica tiene la forma:\n",
    "\n",
    "$$\\displaystyle \\min f_o(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a:} f_i(x) \\leq b_i, i=1,\\dots, m$$\n",
    "\n",
    "donde: $x=(x_1,x_2,\\dots, x_n)^T$ es la **variable de optimización del problema**, la función $f_o: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ es la **función objetivo**, las funciones $f_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}, i=1,\\dots,m$ son las **funciones de restricción** (aquí se colocan únicamente desigualdades pero pueden ser sólo igualdades o bien una combinación de ellas) y las constantes $b_1,b_2,\\dots, b_m$ son los **límites o cotas de las restricciones**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un vector $x^* \\in \\mathbb{R}^n$ es nombrado **óptimo** o solución del problema anterior si tiene el valor más pequeño de entre todos los vectores $x \\in \\mathbb{R}^n$ que satisfacen las restricciones. Por ejemplo, si $z \\in \\mathbb{R}^n$ satisface $f_1(z) \\leq b_1, f_2(z) \\leq b_2, \\dots, f_m(z) \\leq b_m$ y $x^*$ es óptimo entonces $f_o(z) \\geq f_o(x^*)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "A grandes rasgos dos problemas de optimización son equivalentes si con la solución de uno de ellos se obtiene la solución del otro y viceversa.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* Se consideran funciones objetivo $f_o: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, sin embargo, hay formulaciones que utilizan $f_o: \\mathbb{R}^n \\rightarrow \\mathbb{R}^q$. Tales formulaciones pueden hallarlas en la optimización multicriterio, multiobjetivo, vectorial o también nombrada Pareto, ver [Multi objective optimization](https://en.wikipedia.org/wiki/Multi-objective_optimization).\n",
    "\n",
    "* El problema de optimización definido utiliza una forma de minimización y no de maximización. Típicamente en la literatura por convención se consideran problemas de este tipo. Además minimizar $f_o$ y maximizar $-f_o$ son **problemas de optimización equivalentes**.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle \\min_{x \\in \\mathbb{R}^n} ||x||_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a:} Ax \\leq b$$\n",
    "\n",
    "\n",
    "con $A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m$. En este problema buscamos el vector $x$ que es solución del problema $Ax \\leq b$ con **mínima norma Euclidiana**. La función objetivo es $f_o(x)=||x||_2$, las funciones de restricción son las desigualdades lineales $f_i(x) = a_i^Tx \\leq b_i$ con $a_i$ $i$-ésimo renglón de $A$ y $b_i$ $i$-ésima componente de $b$, $\\forall i=1,\\dots,m$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentario\n",
    "\n",
    "Un problema similar (sólo modificando desigualdad por igualdad) lo encontramos al resolver un sistema de ecuaciones lineales $Ax=b$ *underdetermined* en el que $m < n$ y se busca el vector $x$ con mínima norma Euclidiana que satisfaga tal sistema. Este sistema puede tener infinitas soluciones o ninguna solución.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encuentra el punto en la gráfica de $y=x^2$ que es más cercano al punto $P=(1,0)$ bajo la norma Euclidiana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deseamos minimizar la cantidad $||(1,0)-(x,y)||_2$. Además $y = y(x)$ por lo que definiendo la función objetivo $f_o(x) = ||(1,0)-(x,x^2)||_2=||(1-x,-x^2)||_2=\\sqrt{(1-x)^2+x^4}$, el problema de optimización (sin restricciones) es:\n",
    "\n",
    "$$\\displaystyle \\min_{x \\in \\text{dom}f_o}\\sqrt{(1-x)^2+x^4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización numérica en ciencia de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{sidebar} Optimización de código o software\n",
    "\n",
    "La implementación de los métodos o algoritmos en el contexto de grandes cantidades de datos o big data es crítica al ir a la práctica pues de esto depende que nuestra(s) máquina(s) tarde meses, semanas, días u horas para resolver problemas que se presentan en este contexto. En este contexto la [optimización de código o de software](https://en.wikipedia.org/wiki/Program_optimization) nos ayuda a la eficiencia.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ciencia de datos apunta al desarrollo de técnicas y se apoya de aplicaciones de *machine learning* para la extracción de conocimiento útil tomando como fuente de información las grandes cantidades de datos. Algunas de las aplicaciones son:\n",
    "\n",
    "* Clasificación de documentos o textos: detección de *spam*.\n",
    "\n",
    "* [Procesamiento de lenguaje natural](https://en.wikipedia.org/wiki/Natural_language_processing):  [named-entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition).\n",
    "\n",
    "* [Reconocimiento de voz](https://en.wikipedia.org/wiki/Speech_recognition).\n",
    "\n",
    "* [Visión por computadora](https://en.wikipedia.org/wiki/Computer_vision): reconocimiento de rostros o imágenes.\n",
    "\n",
    "* Detección de fraude.\n",
    "\n",
    "* [Reconocimiento de patrones](https://en.wikipedia.org/wiki/Pattern_recognition).\n",
    "\n",
    "* Diagnóstico médico.\n",
    "\n",
    "* [Sistemas de recomendación](https://en.wikipedia.org/wiki/Recommender_system)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las aplicaciones anteriores involucran problemas como son:\n",
    "\n",
    "* Clasificación.\n",
    "\n",
    "* Regresión.\n",
    "\n",
    "* *Ranking*.\n",
    "\n",
    "* *Clustering*.\n",
    "\n",
    "* Reducción de la dimensionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización numérica y *machine learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cada una de las aplicaciones o problemas anteriores se utilizan **funciones de pérdida** que guían el proceso de aprendizaje. Tal proceso involucra **optimización parámetros** de la función de pérdida. Por ejemplo, si la función de pérdida en un problema de regresión es una pérdida cuadrática $\\mathcal{L}(y,\\hat{y}) = (\\hat{y}-y)^2$ con $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta_1}x$, entonces el vector de parámetros a optimizar (aprender) es $\\beta= \\left[ \\begin{array}{c} \\beta_0\\\\ \\beta_1 \\end{array} \\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{sidebar} Un poco de historia...\n",
    "\n",
    "La IA o Inteligencia Artificial es una rama de la Ciencia de la Computación que atrajo un gran interés en 1950.\n",
    "\n",
    "Colloquially, the term artificial intelligence is often used to describe machines (or computers) that mimic “cognitive” functions that humans associate with the human mind, such as learning and problem solving ([S. J. Russel, P. Norvig, 1995](https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Machine learning* no sólo se apoya de la optimización pues es un área de Inteligencia Artificial que utiliza técnicas estadísticas para el diseño de sistemas capaces de aplicaciones como las escritas anteriormente, de modo que hoy en día tenemos *statistical machine learning*. No obstante, uno de los **pilares** de *machine learning* o *statistical machine learning* es la optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Machine learning* o *statistical machine learning* se apoya de las formulaciones y algoritmos en optimización. Sin embargo, también ha contribuido a ésta área desarrollando nuevos enfoques en los métodos o algoritmos para el tratamiento de grandes cantidades de datos o *big data* y estableciendo retos significativos no presentes en problemas clásicos de optimización. De hecho, al revisar literatura que intersecta estas dos disciplinas encontramos comunidades científicas que desarrollan o utilizan métodos o algoritmos exactos (ver [Exact algorithm](https://en.wikipedia.org/wiki/Exact_algorithm)) y otras que utilizan métodos de optimización estocástica (ver [Stochastic optimization](https://en.wikipedia.org/wiki/Stochastic_optimization) y [Stochastic approximation](https://en.wikipedia.org/wiki/Stochastic_approximation)) basados en métodos o algoritmos aproximados (ver [Approximation algorithm](https://en.wikipedia.org/wiki/Approximation_algorithm)). Hoy en día es común encontrar estudios que hacen referencia a **modelos o métodos de aprendizaje**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "Como ejemplo de lo anterior considérese la técnica de [**regularización**](https://en.wikipedia.org/wiki/Regularization_(mathematics)) que en *machine learning* se utiliza para encontrar soluciones que generalicen y provean una explicación no compleja del fenómeno en estudio. \n",
    "\n",
    "La regularización sigue el principio de la navaja de Occam, ver [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor): para cualquier conjunto de observaciones en general se prefieren explicaciones simples a explicaciones más complicadas. Aunque la técnica de regularización es conocida en optimización, han sido varias las aplicaciones de *machine learning* las que la han posicionado como clave.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Del *small scale* al *large scale machine learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{sidebar} Un poco de historia...\n",
    "\n",
    "Un ejemplo de esto se observa en métodos de optimización desarrollados en la década de los $50$'s. Mientras que métodos tradicionales en optimización basados en el cálculo del gradiente y la Hessiana de una función son efectivos para problemas de aprendizaje *small-scale* (en los que  utilizamos un enfoque en ***batch*** o por lote), en el contexto del aprendizaje *large-scale*, el **método de gradiente estocástico** se posicionó en el centro de discusiones a inicios del siglo XXI.\n",
    "\n",
    "El método de gradiente estocástico fue propuesto por Robbins y Monro en 1951, es un **algoritmo estocástico**. Ver [Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El inicio del siglo XXI estuvo marcado, entre otros temas, por un incremento significativo en la generación de información. Esto puede contrastarse con el desarrollo de los procesadores de las máquinas, el cual tuvo un menor *performance* al del siglo XX. Asimismo, las mejoras en dispositivos de almacenamiento o *storage* y sistemas de *networking* abarató costos de almacenamiento y permitió tal incremento de información.  En este contexto, los modelos y métodos de *statistical machine learning* se vieron limitados por el tiempo de cómputo y no por el tamaño de muestra. La conclusión de esto fue una inclinación en la comunidad científica por el diseño o uso de métodos o modelos para procesar grandes cantidades de datos usando recursos computacionales comparativamente menores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Optimización numérica convexa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicaciones de *machine learning* conducen al planteamiento de problemas de optimización convexa y no convexa. Por ejemplo en la aplicación de clasificación de textos, en donde se desea asignar un texto a clases definidas de acuerdo a su contenido (determinar si un documento de texto es sobre un tema), puede formularse un problema convexo a partir de una **función de pérdida convexa**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{sidebar} Un poco de historia...\n",
    "\n",
    "Los tipos de redes neuronales profundas, *deep neural networks*, que han sido mayormente usadas a inicios del siglo XXI son las mismas que las que eran populares en los años $90$'s. El éxito de éstos tipos y su uso primordialmente se debe a la disponibilidad de *larger datasets* y mayores recursos computacionales.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ejemplos de aplicaciones en la **optimización no convexa** están el reconocimiento de voz y reconocimiento de imágenes. El uso de [redes neuronales](https://en.wikipedia.org/wiki/Artificial_neural_network) [profundas](https://en.wikipedia.org/wiki/Deep_learning) ha tenido muy buen desempeño en tales aplicaciones haciendo uso de cómputo en la GPU, ver [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), [2012: A Breakthrough Year for Deep Learning](https://medium.com/limitlessai/2012-a-breakthrough-year-for-deep-learning-2a31a6796e73). En este caso se utilizan **funciones objetivo no lineales y no convexas**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* Desde los $40$'s se han desarrollado algoritmos para resolver problemas de optimización, se han analizado sus propiedades y se han desarrollado buenas implementaciones de software.  Sin embargo, una clase de problemas de optimización en los que encontramos métodos **efectivos** son los convexos. \n",
    "\n",
    "* Métodos para optimización no convexa utilizan parte de la teoría de convexidad desarrollada en optimización convexa. Además, un buen número de problemas de aprendizaje utilizan funciones de pérdida convexas.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(PESTOPT)="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema estándar de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lo que continúa se considera $f_0 = f_o$ (el subíndice \"0\" y el subíndice \"o\" son iguales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "Un problema estándar de optimización es:\n",
    "\n",
    "$$\\displaystyle \\min f_o(x)$$\n",
    "\n",
    "$$\\text{sujeto a:}$$\n",
    "\n",
    "$$f_i(x) \\leq 0, \\quad \\forall i=1,\\dots,m$$\n",
    "\n",
    "$$h_i(x) = 0, \\quad \\forall i=1,\\dots,p$$\n",
    "\n",
    "con $f_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ $\\forall i=0,\\dots,m$, $h_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, $\\forall i=1,\\dots,p$. $f_i$ son las **restricciones de desigualdad**, $h_i$ son las **restricciones de igualdad**.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dominio del problema de optimización y puntos factibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definiciones\n",
    "\n",
    "* El conjunto de puntos para los que la función objetivo y las funciones de restricción $f_i, h_i$ están definidas se nombra **dominio del problema de optimización**, esto es:\n",
    "\n",
    "$$\\mathcal{D} = \\bigcap_{i=0}^m\\text{dom}f_i \\cap \\bigcap_{i=1}^p\\text{dom}h_i.$$\n",
    "\n",
    "\n",
    "* Un punto $x \\in \\mathcal{D}$ se nombra **factible** si satisface las restricciones de igualdad y desigualdad. El conjunto de puntos factibles se nombra **conjunto de factibilidad**.\n",
    "\n",
    "* El {ref}`problema estándar de optimización <PESTOPT>` se nombra **problema de optimización factible** si existe **al menos un punto factible**, si no entonces es infactible.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor óptimo del problema de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "Se asumen todos los puntos en el dominio del problema de optimización $\\mathcal{D}$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "El valor óptimo del problema se denota como $p^*$. En notación matemática es:\n",
    "\n",
    "$$p^* = \\inf\\{f_o(x) | f_i(x) \\leq 0, \\forall i=1,\\dots,m, h_i(x) = 0 \\forall i=1,\\dots,p\\}$$\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* Si el problema es **infactible** entonces $p^* = \\infty$.\n",
    "\n",
    "* Si $\\exists x_k$ factible tal que $f_o(x_k) \\rightarrow -\\infty$ para $k \\rightarrow \\infty$ entonces $p^*=-\\infty$ y se nombra **problema de optimización no acotado por debajo**.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(POPTPROBOPT)="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto óptimo del problema de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "Se asumen todos los puntos en el dominio del problema de optimización $\\mathcal{D}$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "$x^*$ es **punto óptimo** si es factible y $f_o(x^*) = p^*$. \n",
    "\n",
    "El conjunto de óptimos se nombra **conjunto óptimo** y se denota:\n",
    "\n",
    "$$X_{\\text{opt}} = \\{x | f_i(x) \\leq 0 \\forall i=1,\\dots,m, h_i(x) =0 \\forall i=1,\\dots,p, f_o(x) = p^*\\}$$\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* La propiedad de un punto óptimo $x^*$ es que si $z$ satisface las restricciones $f_i(z) \\leq 0$ $\\forall i=1,...,m$, $h_i(z)=0$ $\\forall i=1,..,p$ se tiene: $f(x^*) \\leq f(z)$. Es **óptimo estricto** si $z$ satisface las restricciones y $f_o(x^*) < f_o(z)$.\n",
    "\n",
    "* Si existe un punto óptimo se dice que el **valor óptimo se alcanza** y por tanto el problema de optimización tiene solución, es **soluble o *solvable***.\n",
    "\n",
    "* Si $X_{\\text{opt}} = \\emptyset$ se dice que el valor óptimo no se alcanza. Obsérvese que para problemas no acotados nunca se alcanza el valor óptimo.\n",
    "\n",
    "* Si $x$ es factible y $f_o(x) \\leq p^* + \\epsilon$ con $\\epsilon >0$, $x$ se nombra **$\\epsilon$-subóptimo** y el conjunto de puntos $\\epsilon$-subóptimos se nombra **conjunto $\\epsilon$-subóptimo**.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Óptimo local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "Se asumen todos los puntos en el dominio del problema de optimización $\\mathcal{D}$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "Un punto factible $x^*$ se nombra **óptimo local** si $\\exists R > 0$ tal que:\n",
    "\n",
    "$$f_o(x^*) = \\inf \\{f_o(z) | f_i(z) \\leq 0 \\forall i=1,\\dots,m, h_i(z) = 0 \\forall i=1,\\dots, p, ||z-x||_2 \\leq R\\}.$$\n",
    "\n",
    "Así, $x^*$ resuelve:\n",
    "\n",
    "$$\\displaystyle \\min f_o(z)$$\n",
    "\n",
    "$$\\text{sujeto a:}$$\n",
    "\n",
    "$$f_i(z) \\leq 0, \\forall i =1,\\dots,m$$\n",
    "\n",
    "$$h_i(z) =0, \\forall i=1,\\dots,p$$\n",
    "\n",
    "$$||z-x||_2 \\leq R$$\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "La palabra **óptimo** se utiliza para **óptimo global**, esto es, no consideramos la última restricción $||z-x||_2 \\leq R$ en el problema de optimización y exploramos en todo el $\\text{dom}f$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/xyprhh7erbb6icb/min-max-points-example.png?dl=0\" heigth=\"700\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "Es común referirse al conjunto de mínimos y máximos como puntos extremos de una función.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricciones activas, no activas y redundantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "Se asumen todos los puntos en el dominio del problema de optimización $\\mathcal{D}$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "Si $x$ es factible y $f_i(x)=0$ entonces la restricción de desigualdad $f_i(x) \\leq 0$ se nombra **restricción activa en $x$**. Se nombra **inactiva en $x$** si $f_i(x) <0$ para alguna $i=1,\\dots ,m$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* Las restricciones de igualdad, $h_i(x)$, siempre son activas en el conjunto factible con $i=1,\\dots ,p$. \n",
    "\n",
    "* Una restricción se nombra **restricción redundante** si al quitarla el conjunto factible no se modifica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas de optimización convexa en su forma estándar o canónica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "Se asumen todos los puntos en el dominio del problema de optimización $\\mathcal{D}$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "Recuerda que una función afín es de la forma $h(x) = Ax+b$ con $A \\in \\mathbb{R}^{p \\times n}$ y $b \\in \\mathbb{R}^p$. En la definición $h_i(x) = a_i^Tx-b_i$ con $a_i \\in \\mathbb{R}^n$, $b_i \\in \\mathbb{R}$ $\\forall i=1,\\dots,p$ y geométricamente $h_i(x)$ es un **hiperplano** en $\\mathbb{R}^n$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "Se define un problema de optimización convexa en su forma estándar o canónica como:\n",
    "\n",
    "$$\\displaystyle \\min f_o(x)$$\n",
    "\n",
    "$$\\text{sujeto a:}$$\n",
    "\n",
    "$$f_i(x) \\leq 0 , i=1,\\dots,m$$\n",
    "\n",
    "$$h_i(x)=0, i=1,\\dots,p$$\n",
    "\n",
    "donde: $f_i$ son **convexas** $\\forall i=0,1,\\dots,m$ y $h_i$ **es afín** $\\forall i =1,\\dots,p$. \n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "Un conjunto $\\alpha$-subnivel es de la forma $\\{x \\in \\text{dom}f | f(x) \\leq \\alpha\\}$. Un conjunto subnivel contiene las curvas de nivel de $f$, ver [Level set](https://en.wikipedia.org/wiki/Level_set):\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/0woqoj8foo5eco9/level_set_of_func.png?dl=0\" heigth=\"300\" width=\"300\">\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* El conjunto de factibilidad de un problema de optimización convexa es un conjunto convexo. Esto se sigue pues es una intersección finita de conjuntos convexos: intersección entre las $x$'s que satisfacen $f_i(x) \\leq 0$, $i=1,\\dots ,m$, que se nombra **conjunto subnivel**, y las $x$'s que están en un hiperplano, esto es, que satisfacen $h_i(x) = 0$, $i=1,\\dots ,p$.\n",
    "\n",
    "\n",
    "* Si en el problema anterior se tiene que **maximizar** una $f_o$ función objetivo **cóncava** y se tienen misma forma estándar: $f_i$ convexa, $h_i$ afín entonces también se nombra al problema como **problema de optimización convexa**. Todos los resultados, conclusiones y algoritmos desarrollados para los problemas de minimización son aplicables para maximización. En este caso se puede resolver un problema de maximización al minimizar la función objetivo  $-f_o$ que es convexa.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función convexa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "Sea $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ una función con el conjunto $\\text{dom}f$ convexo. $f$ se nombra convexa  (en su $\\text{dom}f$) si $\\forall x,y \\in \\text{dom}f$ y $\\theta \\in [0,1]$ se cumple:\n",
    "\n",
    "$$f(\\theta x + (1-\\theta) y) \\leq \\theta f(x) + (1-\\theta)f(y).$$\n",
    "\n",
    "Si la desigualdad se cumple de forma estricta $\\forall x \\neq y$ $f$ se nombra **estrictamente convexa**.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observaciones\n",
    ":class: tip\n",
    "\n",
    "* La convexidad de $f$ se define para $\\text{dom}f$ aunque para casos en particular se detalla el conjunto en el que $f$ es convexa.\n",
    "\n",
    "* La desigualdad que define a funciones convexas se nombra [**desigualdad de Jensen**](https://en.wikipedia.org/wiki/Jensen%27s_inequality).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propiedades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre las propiedades que tiene una función convexa se encuentran las siguientes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Si $f$ es convexa el conjunto subnivel es un conjunto convexo. \n",
    "\n",
    "* $\\text{dom}f$ es convexo $\\therefore$ $\\theta x + (1-\\theta)y \\in \\text{dom}f$\n",
    "\n",
    "\n",
    "* $f$ es **cóncava** si $-f$ es convexa y **estrictamente cóncava** si $-f$ es estrictamente convexa. Otra forma de definir concavidad es con una desigualdad del tipo:\n",
    "\n",
    "$$f(\\theta x + (1-\\theta) y) \\geq \\theta f(x) + (1-\\theta)f(y).$$\n",
    "\n",
    "y mismas definiciones para $x,y, \\theta$ que en la definición de convexidad.\n",
    "\n",
    "* Si $f$ es convexa, geométricamente el segmento de línea que se forma con los puntos $(x,f(x)), (y,f(y))$ está por encima o es igual a $f(\\theta x + (1-\\theta)y) \\forall \\theta \\in [0,1]$ y $\\forall x,y \\in \\text{dom}f$:\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/fdcx1k150nfwykv/draw_convexity_for_functions.png?dl=0\" heigth=\"300\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjuntos convexos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Línea y segmentos de línea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "Sean $x_1, x_2 \\in \\mathbb{R}^n$ con $x_1 \\neq x_2$. Entonces el punto:\n",
    "\n",
    "$$y = \\theta x_1 + (1-\\theta)x_2$$\n",
    "\n",
    "con $\\theta \\in \\mathbb{R}$ se encuentra en la línea que pasa por $x_1$ y $x_2$. $\\theta$ se le nombra parámetro y si $\\theta \\in [0,1]$ tenemos un segmento de línea:\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/dldljf5igy8xt9d/segmento_linea.png?dl=0\" heigth=\"200\" width=\"200\">\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* $y = \\theta x_1 + (1-\\theta)x_2 = x_2 + \\theta(x_1 -x_2)$ y esta última igualdad se interpreta como \"$y$ es la suma del punto base $x_2$ y la dirección $x_1-x_2$ escalada por $\\theta$\". \n",
    "\n",
    "* Si $\\theta=0$ entonces $y=x_2$. Si $\\theta \\in [0,1]$ entonces $y$ se \"mueve\" en la dirección $x_1-x_2$ hacia $x_1$ y si $\\theta>1$ entonces $y$ se encuentra en la línea \"más allá\" de $x_1$:\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/nbahrio7p1mj4hs/segmento_linea_2.png?dl=0\" heigth=\"350\" width=\"350\">\n",
    "\n",
    "\n",
    "El punto entre $x_1$ y $x_2$ tiene $\\theta=\\frac{1}{2}$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto convexo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "Un conjunto $\\mathcal{C}$ es convexo si el segmento de línea entre cualquier par de puntos de $\\mathcal{C}$ está completamente contenida en $\\mathcal{C}$. Esto se escribe matemáticamente como:\n",
    "\n",
    "$$\\theta x_1 + (1-\\theta) x_2 \\in \\mathcal{C} \\quad \\forall \\theta \\in [0,1], \\forall x_1, x_2 \\in \\mathcal{C}.$$\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos gráficos de conjuntos convexos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/gj54ism1lqojot6/ej_conj_convexos.png?dl=0\" heigth=\"400\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos gráficos de conjuntos no convexos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/k37zh5v3iq3kx04/ej_conj_no_convexos.png?dl=0\" heigth=\"350\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "\n",
    "* El punto $\\displaystyle \\sum_{i=1}^k \\theta_i x_i$ con $\\displaystyle \\sum_{i=1}^k \\theta_i=1$, $\\theta_i \\geq 0 \\forall i=1,\\dots,k$ se nombra **combinación convexa** de los puntos $x_1, x_2, \\dots, x_k$. Una combinación convexa de los puntos $x_1, \\dots, x_k$ puede pensarse como una mezcla o promedio ponderado de los puntos, con $\\theta_i$ la fracción $\\theta_i$ de $x_i$ en la mezcla.\n",
    "\n",
    "* Un conjunto es convexo si y sólo si contiene cualquier combinación convexa de sus puntos.\n",
    "\n",
    "* El conjunto óptimo y los conjuntos $\\epsilon$-subóptimos son convexos. Ver definiciones de conjunto óptimo y $\\epsilon$-subóptimos en {ref}`punto óptimo del problema de optimización<POPTPROBOPT>`.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos de funciones convexas y cóncavas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una función afín es convexa y cóncava en todo su dominio: $f(x) = Ax+b$ con $A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^n$, $\\text{dom}f = \\mathbb{R}^n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "Por tanto las funciones lineales también son convexas y cóncavas.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Funciones cuadráticas: $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, $f(x) = \\frac{1}{2} x^TPx + q^Tx + r$ son convexas en su dominio $\\mathbb{R}^n$ si $P \\in \\mathcal{S}_+^n, q \\in \\mathbb{R}^n, r \\in \\mathbb{R}$ con $\\mathbb{S}_+^n$ conjunto de **matrices simétricas positivas semidefinidas**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "Una matriz $A$ es positiva semidefinida si $x^TAx \\geq 0$ $\\forall x \\in \\mathbb{R}^n - \\{0\\}$. Si se cumple de forma estricta la desigualdad entonces $A$ es **positiva definida**. Con los eigenvalores podemos caracterizar a las matrices definidas y semidefinidas positivas: $A$ es semidefinida positiva si y sólo si los eigenvalores de $T=\\frac{A+A^T}{2}$ son no negativos. Es definida positiva si y sólo si los eigenvalores de $T$ son positivos. Los conjuntos de matrices que se utilizan para definir a matrices semidefinidas positivas y definidas positivas son $\\mathbb{S}_{+}^n$ y $\\mathbb{S}_{++}^n$ respectivamente ($\\mathbb{S}$ es el conjunto de matrices simétricas).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentario\n",
    "\n",
    "$f$ es estrictamente convexa si y sólo si $P \\in \\mathbb{S}_{++}^n$. $f$ es cóncava si y sólo si $P \\in -\\mathbb{S}_+^n$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exponenciales: $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, $f(x) = e^{ax}$ para cualquier $a \\in \\mathbb{R}$ es convexa en su dominio $\\mathbb{R}$.\n",
    "\n",
    "* Potencias: $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, $f(x)=x^a$:\n",
    "\n",
    "    * Si $a \\geq 1$ o $a \\leq 0$ entonces $f$ es convexa en $\\mathbb{R}_{++}$ (números reales positivos).\n",
    "    * Si $0 \\leq a \\leq 1$ entonces $f$ es cóncava en $\\mathbb{R}_{++}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Potencias del valor absoluto: $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, $f(x)=|x|^p$ con $p \\geq 1$ es convexa en $\\mathbb{R}$.\n",
    "\n",
    "* Logaritmo: $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, $f(x) = \\log(x)$ es cóncava en su dominio: $\\mathbb{R}_{++}$.\n",
    "\n",
    "* Entropía negativa: $f(x) = \\begin{cases}\n",
    "x\\log(x) &\\text{ si } x > 0 ,\\\\\n",
    "0 &\\text{ si } x = 0\n",
    "\\end{cases}$ es estrictamente convexa en su dominio $\\mathbb{R}_+$.\n",
    "\n",
    "* Normas: cualquier norma es convexa en su dominio.\n",
    "\n",
    "* Función máximo: $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$, $f(x) = \\max\\{x_1,\\dots,x_n\\}$ es convexa.\n",
    "\n",
    "* Función log-sum-exp: $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$, $f(x)=\\log\\left(\\displaystyle \\sum_{i=1}^ne^{x_i}\\right)$ es convexa en su dominio $\\mathbb{R}^n$.\n",
    "\n",
    "* La media geométrica: $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$, $f(x) = \\left(\\displaystyle \\prod_{i=1}^n x_i \\right)^\\frac{1}{n}$ es cóncava en su dominio $\\mathbb{R}_{++}^n$.\n",
    "\n",
    "* Función log-determinante: $f: \\mathbb{S}^{n} \\rightarrow \\mathbb{R}^n$, $f(x) = \\log(\\det(X))$ es cóncava en su dominio $\\mathbb{S}_{++}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(RESUTTEOCONV)="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados útiles de teoría de convexidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "Se sugiere revisar {ref}`definición de función, continuidad y derivada <FCD>` y {ref}`condición de un problema y estabilidad de un algoritmo <CPEA>` como recordatorio de definiciones. En particular las **definiciones de primera y segunda derivada, gradiente y Hessiana** para la primer nota y la **definición de número de condición de una matriz** para la segunda.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobre funciones convexas/cóncavas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Sea $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ diferenciable entonces $f$ es convexa si y sólo si $\\text{dom}f$ es un conjunto convexo y se cumple:\n",
    "\n",
    "$$f(y) \\geq f(x) + \\nabla f(x)^T(y-x) \\forall x,y \\in \\text{dom}f.$$\n",
    "\n",
    "Si se cumple de forma estricta la desigualdad $f$ se nombra estrictamente convexa. También si su $\\text{dom}f$ es convexo y se tiene la desigualdad en la otra dirección \"$\\leq$\" entonces $f$ es cóncava.\n",
    "\n",
    "Geométricamente este resultado se ve como sigue para $\\nabla f(x) \\neq 0$:\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/e581e22xeejdwu0/convexidad_con_hiperplano_de_soporte.png?dl=0\" heigth=\"350\" width=\"350\">\n",
    "\n",
    "\n",
    "y el hiperplano $f(x) + \\nabla f(x)^T(y-x)$ se nombra **hiperplano de soporte para la función $f$ en el punto $(x,f(x))$**. Obsérvese que si $\\nabla f(x)=0$ se tiene $f(y) \\geq f(x) \\forall y \\in \\text{dom}f$ y por lo tanto $x$ es un mínimo global de $f$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una función es convexa si y sólo si es convexa al restringirla a cualquier línea que intersecte su dominio, esto es, si $g(t) = f(x + tv)$ es convexa $\\forall x,v \\in \\mathbb{R}^n$, $\\forall t \\in \\mathbb{R}$ talque $x + tv \\in \\text{dom}f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sea $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ tal que $f \\in \\mathcal{C}^2(\\text{dom}f)$. Entonces $f$ es convexa en $\\text{dom}f$ si y sólo si $\\text{dom}f$ es convexo y $\\nabla^2f(x) \\in \\mathbb{S}^n_+$ en $\\text{dom}f$. Si $\\nabla^2f(x) \\in \\mathbb{S}^n_{++}$ en $\\text{dom}f$ y $\\text{dom}f$ es convexo entonces $f$ es estrictamente convexa en $\\text{dom}f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentario\n",
    "\n",
    "Para una función: $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, la hipótesis del enunciado anterior ($\\nabla^2 f(x) \\in \\mathbb{S}^n_{++}$ en $\\text{dom}f$) es que la segunda derivada sea positiva. El recíproco no es verdadero, para ver esto considérese $f(x)=x^4$ la cual es estrictamente convexa en $\\text{dom}f$ pero su segunda derivada en $0$ no es positiva.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobre problemas de optimización\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para problemas de optimización sin restricciones:\n",
    "\n",
    "\n",
    "* Si $f$ es diferenciable y $x^*$ es óptimo entonces $\\nabla f(x^*) = 0$.\n",
    "\n",
    "* Si $f \\in \\mathcal{C}^2(\\text{domf})$ y $x^*$ es mínimo local entonces $\\nabla^2 f(x^*)$ es una matriz simétrica semidefinida positiva.\n",
    "\n",
    "* Si $f \\in \\mathcal{C}^2(\\text{domf})$, $\\nabla f(x^*)=0$ y $\\nabla^2f(x^*)$ es una matriz simétrica definida positiva entonces $x^*$ es mínimo local estricto.\n",
    "\n",
    "Para problemas de optimización convexa:\n",
    "\n",
    "* Una propiedad fundamental de un óptimo local en un problema de optimización convexa es que también es un óptimo global. Si la función es estrictamente convexa entonces el conjunto óptimo contiene a lo más un punto.\n",
    "\n",
    "* Si $f_o$ es diferenciable y $X$ es el conjunto de factibilidad entonces $x$ es óptimo si y sólo si $x \\in X$ y $\\nabla f_o(x)^T(y-x) \\geq 0$ $\\forall y \\in X$. Si se considera como conjunto de factibilidad $X = \\text{dom}f_o$ (que es un problema sin restricciones) la propiedad se reduce a la condición: $x$ es óptimo si y sólo si $\\nabla f_o(x) = 0$.\n",
    "\n",
    "Geométricamente el resultado anterior se visualiza para $\\nabla f(x) \\neq 0$ y $-\\nabla f(x)$ apuntando hacia la dirección dibujada:\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/0tmpivvo5ob4oox/optimo_convexidad_con_hiperplano_de_soporte.png?dl=0\" heigth=\"550\" width=\"550\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentario\n",
    "\n",
    "Por los resultados anteriores los métodos de optimización buscan resolver la **ecuación no lineal** $\\nabla f_o(x)=0$ para aproximar en general mínimos locales. Dependiendo del número de soluciones de la ecuación $\\nabla f_o(x)=0$ se tienen situaciones distintas. Por ejemplo, si no tiene solución entonces el/los óptimos no se alcanza(n) pues el problema puede no ser acotado por debajo o si existe el óptimo éste puede no alcanzarse. Por otro lado, si la ecuación tiene múltiples soluciones entonces cada solución es un mínimo de $f_o$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(SPCRITICOS)="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobre puntos críticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "Puntos $x \\in \\text{intdom}f$ en los que $\\nabla f(x) = 0$  o en los que $\\nabla f$ no existe, se les nombra **puntos críticos o estacionarios** de $f$. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No todo punto crítico es un extremo de $f$.\n",
    "\n",
    "* La Hessiana de $f$ nos ayuda a caracterizar los puntos críticos en mínimos o máximos locales. Si $x \\in \\mathbb{R}^n$ es punto crítico:\n",
    "\n",
    "    * Y además $\\nabla^2f(x) \\in \\mathbb{S}_{++}$ entonces $x$ es mínimo local.\n",
    "    * Y además $\\nabla^2f(x) \\in -\\mathbb{S}_{++}$ entonces $x$ es máximo local.\n",
    "    * Y además $\\nabla^2f(x)$ es indefinida entonces $x$ se nombra punto silla o [*saddle point*](https://en.wikipedia.org/wiki/Saddle_point).\n",
    "    \n",
    "* Si $x \\in \\mathbb{R}^n$ es punto crítico y $\\nabla^2f(x) \\in \\mathbb{S}_{+}$ no podemos concluir si es máximo o mínimo local (análogo si $\\nabla^2f(x) \\in -\\mathbb{S}_{+}$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función fuertemente convexa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "Una función $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ tal que $f \\in \\mathcal{C}^2(\\text{dom}f)$ se nombra **fuertemente convexa** en el conjunto convexo $\\mathcal{S} \\neq \\emptyset$ si existe $m>0$ tal que $\\nabla^2 f(x) - mI$ es simétrica semidefinida positiva $\\forall x \\in \\mathcal{S}$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* Es equivalente escribir que una función $f$ es fuertemente convexa en un conjunto $\\mathcal{S}$ que escribir $\\lambda_{\\text{min}} (\\nabla^2 f(x))$ es positivo para toda $x \\in \\mathcal{S}$.\n",
    "\n",
    "* Si una función es fuertemente convexa se puede probar que:\n",
    "\n",
    "    * El conjunto óptimo contiene a lo más un punto.\n",
    "\n",
    "\n",
    "    * $f(y) \\geq f(x) + \\nabla f(x)^T(y-x) + \\frac{m}{2}||y-x||_2^2 \\forall x,y \\in \\mathcal{S}$. Por esto si $f$ es fuertemente convexa en $\\mathcal{S}$ entonces es estrictamente convexa en $\\mathcal{S}$. También esta desigualdad indica que la diferencia entre la función de $y$, $f(y)$, y la función lineal en $y$ $f(x) + \\nabla f(x)^T(y-x)$ (Taylor a primer orden) está acotada por debajo por una cantidad cuadrática.\n",
    "\n",
    "    * Existe una cota superior para el **número de condición** bajo la norma 2 de la Hessiana de $f$, esto es: $\\nabla ^2 f(x)$= $\\frac{\\lambda_\\text{max}(\\nabla^2 f(x))}{\\lambda_\\text{min}(\\nabla^2 f(x))} \\leq K$ con $K>0$, $\\forall x \\in \\mathcal{S}$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observaciones\n",
    ":class: tip\n",
    "\n",
    "* La condición que una función sea fuertemente convexa garantiza que el número de condición de la Hessiana de $f$ es una buena medida del desempeño de los algoritmos de optimización convexa sin restricciones (se revisará más adelante).\n",
    "\n",
    "* $f$ es fuertemente convexa en $\\mathcal{S}$ entonces es estrictamente convexa en $\\mathcal{S}$ pero no viceversa, considérese por ejemplo $f=x^4$ la cual es estrictamente convexa en todo su dominio pero no es fuertemente convexa en todo su dominio pues su segunda derivada se anula en $x=0$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas de comprehensión.**\n",
    "\n",
    "1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias:**\n",
    "\n",
    "1. S. P. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University Press, 2009.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
