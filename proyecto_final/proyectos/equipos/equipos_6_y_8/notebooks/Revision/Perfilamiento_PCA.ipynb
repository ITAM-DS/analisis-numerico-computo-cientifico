{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfilamiento del Código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías, funciones y datos para las pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --user line_profiler\n",
    "%pip install -q --user memory_profiler\n",
    "%pip install -q --user psutil\n",
    "%pip install -q --user guppy3\n",
    "#%pip install -q --user pkgcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import platform\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from guppy import hpy\n",
    "from scipy.integrate import quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importamos el módulo con nuestras funciones\n",
    "import sys\n",
    "sys.path.append('./../../')\n",
    "\n",
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "#Own Library modules\n",
    "import src\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2\n",
    "\n",
    "from src.pca import todoJunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/nndb_flat.csv', encoding = \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.columns[df.columns.str.contains('_USRDA')].values, \n",
    "        inplace=True, axis=1)\n",
    "df = df.drop(columns=['ID','FoodGroup','ShortDescrip','Descrip','CommonName','MfgName','ScientificName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Se estandarizan los datos.\n",
    "scaler = preprocessing.StandardScaler()\n",
    "df_nutrientes = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equipo Utilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== Información del Sistema ========================================\n",
      "Sistema: Windows\n",
      "Nombre: DESKTOP-F8PSOT5\n",
      "OS: 10\n",
      "Versión: 10.0.17763\n",
      "Máquina: AMD64\n",
      "Procesador: Intel64 Family 6 Model 158 Stepping 9, GenuineIntel\n",
      "======================================== Memoria ========================================\n",
      "Total: 15.88GB\n",
      "Desponible: 6.28GB\n",
      "Utilizada: 9.59GB\n",
      "Porcentaje: 60.4%\n",
      "======================================== CPU ========================================\n",
      "Cores Físicos: 4\n",
      "Cores Totales: 4\n",
      "Frecuencia del CPU : scpufreq(current=2501.0, min=0.0, max=2501.0)%\n",
      "======================================== Discos Duros ========================================\n",
      "Particiones:\n",
      "=== Dispositivo: C:\\ ===\n",
      "  Montura: C:\\\n",
      "  Tipo de Sistema de Archivos: NTFS\n",
      "  Tamaño Total: 1.68TB\n",
      "  Usado: 1.19TB\n",
      "  Libre: 502.10GB\n",
      "  Porcentaje: 70.8%\n",
      "=== Dispositivo: D:\\ ===\n",
      "  Montura: D:\\\n",
      "  Tipo de Sistema de Archivos: NTFS\n",
      "  Tamaño Total: 25.00GB\n",
      "  Usado: 2.16GB\n",
      "  Libre: 22.84GB\n",
      "  Porcentaje: 8.7%\n"
     ]
    }
   ],
   "source": [
    "#Función para escalar cifras en Bytes a las unidades correspondientes\n",
    "def get_size(bytes, suffix=\"B\"):\n",
    "\n",
    "    factor = 1024\n",
    "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
    "        if bytes < factor:\n",
    "            return f\"{bytes:.2f}{unit}{suffix}\"\n",
    "        bytes /= factor\n",
    "\n",
    "print(\"=\"*40, \"Información del Sistema\", \"=\"*40)\n",
    "uname = platform.uname()\n",
    "print(f\"Sistema: {uname.system}\")\n",
    "print(f\"Nombre: {uname.node}\")\n",
    "print(f\"OS: {uname.release}\")\n",
    "print(f\"Versión: {uname.version}\")\n",
    "print(f\"Máquina: {uname.machine}\")\n",
    "print(f\"Procesador: {uname.processor}\")\n",
    "\n",
    "# Detalles de la memoria\n",
    "print(\"=\"*40, \"Memoria\", \"=\"*40)\n",
    "svmem = psutil.virtual_memory()\n",
    "print(f\"Total: {get_size(svmem.total)}\")\n",
    "print(f\"Desponible: {get_size(svmem.available)}\")\n",
    "print(f\"Utilizada: {get_size(svmem.used)}\")\n",
    "print(f\"Porcentaje: {svmem.percent}%\")\n",
    "print(\"=\"*40, \"CPU\", \"=\"*40)\n",
    "# number of cores\n",
    "print(\"Cores Físicos:\", psutil.cpu_count(logical=False))\n",
    "print(\"Cores Totales:\", psutil.cpu_count(logical=True))\n",
    "# CPU frequencies\n",
    "cpufreq = psutil.cpu_freq()\n",
    "print(f\"Frecuencia del CPU : {psutil.cpu_freq()}%\")\n",
    "# Disk Information\n",
    "print(\"=\"*40, \"Discos Duros\", \"=\"*40)\n",
    "print(\"Particiones:\")\n",
    "# get all disk partitions\n",
    "partitions = psutil.disk_partitions()\n",
    "for partition in partitions:\n",
    "    print(f\"=== Dispositivo: {partition.device} ===\")\n",
    "    print(f\"  Montura: {partition.mountpoint}\")\n",
    "    print(f\"  Tipo de Sistema de Archivos: {partition.fstype}\")\n",
    "    try:\n",
    "        partition_usage = psutil.disk_usage(partition.mountpoint)\n",
    "    except PermissionError:\n",
    "        # this can be catched due to the disk that\n",
    "        # isn't ready\n",
    "        continue\n",
    "    print(f\"  Tamaño Total: {get_size(partition_usage.total)}\")\n",
    "    print(f\"  Usado: {get_size(partition_usage.used)}\")\n",
    "    print(f\"  Libre: {get_size(partition_usage.free)}\")\n",
    "    print(f\"  Porcentaje: {partition_usage.percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pruebas de Tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este notebook es perfilar y los resultados de PCA de cada una de nuestras funciones:\n",
    "+ PCA usando la función SVD de numpy.\n",
    "+ PCA a partir del algoritmo QR\n",
    "+ PCA a partir del método de la potencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 A través de time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time1 = time.time()\n",
    "todoJunto.PCA_from_sklearn(df_nutrientes)\n",
    "end_time1 = time.time()\n",
    "\n",
    "start_time2 = time.time()\n",
    "todoJunto.PCA_from_SVD(df_nutrientes)\n",
    "end_time2 = time.time()\n",
    "\n",
    "start_time3 = time.time()\n",
    "todoJunto.PCA_from_QR_vf(df_nutrientes)\n",
    "end_time3 = time.time()\n",
    "\n",
    "start_time4 = time.time()\n",
    "todoJunto.PCA_from_potencia(df_nutrientes)\n",
    "end_time4 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA_from_sklearn tomó 0.015001058578491211 segundos\n",
      "PCA_from_SVD tomó 0.006003618240356445 segundos\n",
      "PCA_from_QR tomó 1.2579970359802246 segundos\n",
      "PCA_from_potencia tomó 0.4570002555847168 segundos\n"
     ]
    }
   ],
   "source": [
    "secs1 = end_time1-start_time1\n",
    "secs2 = end_time2-start_time2\n",
    "secs3 = end_time3-start_time3\n",
    "secs4 = end_time4-start_time4\n",
    "print(\"PCA_from_sklearn tomó\",secs1,\"segundos\" )\n",
    "print(\"PCA_from_SVD tomó\",secs2,\"segundos\" )\n",
    "print(\"PCA_from_QR tomó\",secs3,\"segundos\" )\n",
    "print(\"PCA_from_potencia tomó\",secs4,\"segundos\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones provenientes de sklearn y numpy fueron claramente más rápidas que las que elaboramos para el proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 A través de %timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en las notas, se utilizará un loop de tamaño $5$ y $20$ repeticiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.4 ms ± 506 µs per loop (mean ± std. dev. of 20 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 5 -r 20 todoJunto.PCA_from_sklearn(df_nutrientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se observan variaciones notorias al correrlo con 20 repeticiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.42 ms ± 483 µs per loop (mean ± std. dev. of 20 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 5 -r 20 todoJunto.PCA_from_SVD(df_nutrientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se observan variaciones notorias al correrlo con 20 repeticiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24 s ± 69.8 ms per loop (mean ± std. dev. of 20 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 5 -r 20 todoJunto.PCA_from_QR_vf(df_nutrientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se observan variaciones notorias al correrlo con 20 repeticiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435 ms ± 9.12 ms per loop (mean ± std. dev. of 20 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 5 -r 20 todoJunto.PCA_from_potencia(df_nutrientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se observan variaciones notorias al correrlo con 20 repeticiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que con time, las funciones provenientes de sklearn y numpy fueron claramente más rápidas que las que elaboramos para el proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Pruebas de uso de CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Con %prun Y %lprun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se perfila primero con *%prun* a cada función  aquellas funciones que gastan un mayor tiempo de ejecución y posteriormente perfilarlas con line_profiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA_from_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         2750 function calls (2677 primitive calls) in 0.022 seconds\n",
       "\n",
       "   Ordered by: cumulative time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.000    0.000    0.022    0.022 {built-in method builtins.exec}\n",
       "        1    0.000    0.000    0.022    0.022 <string>:1(<module>)\n",
       "        1    0.000    0.000    0.021    0.021 todoJunto.py:20(PCA_from_sklearn)\n",
       "        2    0.001    0.000    0.021    0.010 _pca.py:347(fit_transform)\n",
       "        2    0.000    0.000    0.020    0.010 _pca.py:381(_fit)\n",
       "        2    0.000    0.000    0.015    0.008 _pca.py:423(_fit_full)\n",
       "        2    0.011    0.005    0.011    0.006 decomp_svd.py:16(svd)\n",
       "        2    0.000    0.000    0.005    0.003 validation.py:350(check_array)\n",
       "        2    0.003    0.001    0.003    0.002 extmath.py:496(svd_flip)\n",
       "    34/32    0.002    0.000    0.002    0.000 {built-in method numpy.array}\n",
       "       10    0.000    0.000    0.002    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
       "        6    0.000    0.000    0.001    0.000 generic.py:5519(dtypes)\n",
       "       11    0.001    0.000    0.001    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "        6    0.000    0.000    0.001    0.000 series.py:183(__init__)\n",
       "        2    0.000    0.000    0.001    0.000 validation.py:37(_assert_all_finite)\n",
       "        2    0.000    0.000    0.001    0.000 extmath.py:681(_safe_accumulator_op)\n",
       "        2    0.000    0.000    0.001    0.000 <__array_function__ internals>:2(sum)\n",
       "        2    0.000    0.000    0.001    0.000 fromnumeric.py:2092(sum)\n",
       "        2    0.000    0.000    0.001    0.000 fromnumeric.py:73(_wrapreduction)\n",
       "        4    0.000    0.000    0.001    0.000 generic.py:1908(__array__)\n",
       "        4    0.000    0.000    0.001    0.000 {pandas._libs.lib.values_from_object}\n",
       "        4    0.000    0.000    0.001    0.000 generic.py:5412(values)\n",
       "        2    0.000    0.000    0.001    0.000 <__array_function__ internals>:2(mean)\n",
       "        3    0.000    0.000    0.001    0.000 _methods.py:134(_mean)\n",
       "    84/64    0.000    0.000    0.001    0.000 {built-in method builtins.hasattr}\n",
       "        2    0.000    0.000    0.001    0.000 fromnumeric.py:3231(mean)\n",
       "        4    0.000    0.000    0.001    0.000 managers.py:798(as_array)\n",
       "    15/13    0.000    0.000    0.001    0.000 _asarray.py:16(asarray)\n",
       "        6    0.000    0.000    0.000    0.000 construction.py:388(sanitize_array)\n",
       "        2    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(argmax)\n",
       "        2    0.000    0.000    0.000    0.000 _util.py:200(_asarray_validated)\n",
       "        2    0.000    0.000    0.000    0.000 fromnumeric.py:1112(argmax)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:414(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 fromnumeric.py:55(_wrapfunc)\n",
       "        8    0.000    0.000    0.000    0.000 managers.py:199(_is_single_block)\n",
       "  163/116    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'argmax' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 construction.py:123(init_ndarray)\n",
       "        2    0.000    0.000    0.000    0.000 function_base.py:435(asarray_chkfinite)\n",
       "        6    0.000    0.000    0.000    0.000 managers.py:248(get_dtypes)\n",
       "        7    0.000    0.000    0.000    0.000 blocks.py:3021(make_block)\n",
       "       10    0.000    0.000    0.000    0.000 managers.py:314(__len__)\n",
       "      504    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "        6    0.000    0.000    0.000    0.000 managers.py:1467(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 construction.py:506(_try_cast)\n",
       "        6    0.000    0.000    0.000    0.000 algorithms.py:1565(take_nd)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:1643(create_block_manager_from_blocks)\n",
       "        7    0.000    0.000    0.000    0.000 blocks.py:2975(get_block_type)\n",
       "        2    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(may_share_memory)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:122(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 cast.py:1218(maybe_cast_to_datetime)\n",
       "       25    0.000    0.000    0.000    0.000 common.py:222(is_object_dtype)\n",
       "       35    0.000    0.000    0.000    0.000 base.py:247(is_dtype)\n",
       "       11    0.000    0.000    0.000    0.000 managers.py:232(items)\n",
       "  260/258    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
       "       55    0.000    0.000    0.000    0.000 common.py:1708(_is_dtype_type)\n",
       "       35    0.000    0.000    0.000    0.000 common.py:1565(is_extension_array_dtype)\n",
       "       22    0.000    0.000    0.000    0.000 abc.py:137(__instancecheck__)\n",
       "        4    0.000    0.000    0.000    0.000 blocks.py:225(get_values)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:212(_rebuild_blknos_and_blklocs)\n",
       "        6    0.000    0.000    0.000    0.000 algorithms.py:1436(_get_take_nd_function)\n",
       "      160    0.000    0.000    0.000    0.000 generic.py:10(_check)\n",
       "        1    0.000    0.000    0.000    0.000 construction.py:417(_get_axes)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:5388(default_index)\n",
       "       22    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
       "       16    0.000    0.000    0.000    0.000 generic.py:5257(__getattr__)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
       "       19    0.000    0.000    0.000    0.000 _dtype.py:333(_name_get)\n",
       "       12    0.000    0.000    0.000    0.000 construction.py:337(extract_array)\n",
       "       10    0.000    0.000    0.000    0.000 generic.py:5276(__setattr__)\n",
       "        2    0.000    0.000    0.000    0.000 range.py:83(__new__)\n",
       "       29    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1009(_handle_fromlist)\n",
       "        7    0.000    0.000    0.000    0.000 blocks.py:118(__init__)\n",
       "       13    0.000    0.000    0.000    0.000 common.py:403(is_datetime64tz_dtype)\n",
       "        6    0.000    0.000    0.000    0.000 blocks.py:2585(__init__)\n",
       "       41    0.000    0.000    0.000    0.000 dtypes.py:75(find)\n",
       "        2    0.000    0.000    0.000    0.000 validation.py:136(_num_samples)\n",
       "       18    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_list_like}\n",
       "        6    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_1d_object_object}\n",
       "        7    0.000    0.000    0.000    0.000 common.py:339(is_categorical)\n",
       "       19    0.000    0.000    0.000    0.000 common.py:1844(pandas_dtype)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 lapack.py:784(_compute_lwork)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:655(_consolidate_check)\n",
       "       12    0.000    0.000    0.000    0.000 common.py:372(is_datetime64_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 _methods.py:47(_all)\n",
       "        6    0.000    0.000    0.000    0.000 generic.py:255(_validate_dtype)\n",
       "        7    0.000    0.000    0.000    0.000 common.py:506(is_interval_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1035(__iter__)\n",
       "        9    0.000    0.000    0.000    0.000 common.py:542(is_categorical_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:656(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(result_type)\n",
       "        3    0.000    0.000    0.000    0.000 _methods.py:36(_sum)\n",
       "        1    0.000    0.000    0.000    0.000 blocks.py:343(ftype)\n",
       "        6    0.000    0.000    0.000    0.000 cast.py:1097(maybe_castable)\n",
       "        6    0.000    0.000    0.000    0.000 construction.py:570(is_empty_data)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:5341(_consolidate_inplace)\n",
       "       10    0.000    0.000    0.000    0.000 base.py:5394(maybe_extract_name)\n",
       "        7    0.000    0.000    0.000    0.000 dtypes.py:1124(is_dtype)\n",
       "        7    0.000    0.000    0.000    0.000 common.py:472(is_period_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:3950(_can_hold_identifiers_and_holds_name)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:5331(_protect_consolidate)\n",
       "        7    0.000    0.000    0.000    0.000 generic.py:190(__init__)\n",
       "      175    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
       "        7    0.000    0.000    0.000    0.000 common.py:252(is_sparse)\n",
       "       49    0.000    0.000    0.000    0.000 common.py:206(classes)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "        7    0.000    0.000    0.000    0.000 dtypes.py:917(is_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 _dtype.py:46(__str__)\n",
       "       49    0.000    0.000    0.000    0.000 common.py:208(<lambda>)\n",
       "       37    0.000    0.000    0.000    0.000 range.py:675(__len__)\n",
       "        2    0.000    0.000    0.000    0.000 range.py:131(_simple_new)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1679(is_object)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:5344(f)\n",
       "        5    0.000    0.000    0.000    0.000 _asarray.py:88(asanyarray)\n",
       "        2    0.000    0.000    0.000    0.000 warnings.py:165(simplefilter)\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method numpy.empty}\n",
       "        3    0.000    0.000    0.000    0.000 numerictypes.py:365(issubdtype)\n",
       "        6    0.000    0.000    0.000    0.000 series.py:428(name)\n",
       "        6    0.000    0.000    0.000    0.000 common.py:1401(is_float_dtype)\n",
       "        6    0.000    0.000    0.000    0.000 series.py:376(_set_axis)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:329(_verify_integrity)\n",
       "       48    0.000    0.000    0.000    0.000 validation.py:474(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
       "        6    0.000    0.000    0.000    0.000 common.py:775(is_integer_dtype)\n",
       "        7    0.000    0.000    0.000    0.000 blocks.py:251(mgr_locs)\n",
       "       22    0.000    0.000    0.000    0.000 inference.py:358(is_hashable)\n",
       "        2    0.000    0.000    0.000    0.000 warnings.py:181(_add_filter)\n",
       "        1    0.000    0.000    0.000    0.000 _methods.py:44(_any)\n",
       "        6    0.000    0.000    0.000    0.000 cast.py:1492(construct_1d_ndarray_preserving_na)\n",
       "       19    0.000    0.000    0.000    0.000 _dtype.py:319(_name_includes_bit_suffix)\n",
       "        3    0.000    0.000    0.000    0.000 _methods.py:50(_count_reduce_items)\n",
       "        3    0.000    0.000    0.000    0.000 managers.py:163(shape)\n",
       "        8    0.000    0.000    0.000    0.000 generic.py:491(_info_axis)\n",
       "        6    0.000    0.000    0.000    0.000 common.py:441(is_timedelta64_dtype)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:1192(isspmatrix)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method numpy.arange}\n",
       "        2    0.000    0.000    0.000    0.000 blas.py:372(getter)\n",
       "        2    0.000    0.000    0.000    0.000 lapack.py:817(_check_work_float)\n",
       "        6    0.000    0.000    0.000    0.000 series.py:480(_values)\n",
       "        6    0.000    0.000    0.000    0.000 series.py:432(name)\n",
       "        6    0.000    0.000    0.000    0.000 numerictypes.py:293(issubclass_)\n",
       "        6    0.000    0.000    0.000    0.000 managers.py:249(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 abc.py:141(__subclasscheck__)\n",
       "        2    0.000    0.000    0.000    0.000 warnings.py:474(__enter__)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1186(name)\n",
       "        2    0.000    0.000    0.000    0.000 series.py:548(__len__)\n",
       "        2    0.000    0.000    0.000    0.000 _pca.py:316(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 series.py:414(dtype)\n",
       "        4    0.000    0.000    0.000    0.000 managers.py:927(consolidate)\n",
       "        6    0.000    0.000    0.000    0.000 inference.py:96(is_iterator)\n",
       "       19    0.000    0.000    0.000    0.000 _dtype.py:36(_kind_name)\n",
       "        2    0.000    0.000    0.000    0.000 warnings.py:493(__exit__)\n",
       "        6    0.000    0.000    0.000    0.000 managers.py:1565(internal_values)\n",
       "        9    0.000    0.000    0.000    0.000 managers.py:165(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 construction.py:260(prep_ndarray)\n",
       "       25    0.000    0.000    0.000    0.000 blocks.py:247(mgr_locs)\n",
       "        8    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:416(parent)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:188(all_none)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
       "        9    0.000    0.000    0.000    0.000 managers.py:167(ndim)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:5294(ensure_index)\n",
       "        2    0.000    0.000    0.000    0.000 managers.py:1548(dtype)\n",
       "        2    0.000    0.000    0.000    0.000 validation.py:343(_ensure_no_complex_data)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
       "        2    0.000    0.000    0.000    0.000 _config.py:13(get_config)\n",
       "        4    0.000    0.000    0.000    0.000 common.py:179(ensure_python_int)\n",
       "        2    0.000    0.000    0.000    0.000 warnings.py:453(__init__)\n",
       "        9    0.000    0.000    0.000    0.000 blocks.py:339(dtype)\n",
       "        6    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_int64}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
       "       22    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1682(is_categorical)\n",
       "        7    0.000    0.000    0.000    0.000 blocks.py:129(_check_ndim)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'remove' of 'list' objects}\n",
       "        5    0.000    0.000    0.000    0.000 managers.py:647(is_consolidated)\n",
       "        2    0.000    0.000    0.000    0.000 range.py:316(dtype)\n",
       "        6    0.000    0.000    0.000    0.000 series.py:403(_set_subtyp)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:128(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 core.py:6293(isMaskedArray)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'fill' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.sum}\n",
       "        2    0.000    0.000    0.000    0.000 fromnumeric.py:74(<dictcomp>)\n",
       "        6    0.000    0.000    0.000    0.000 common.py:216(<lambda>)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:943(_consolidate_inplace)\n",
       "        8    0.000    0.000    0.000    0.000 managers.py:1520(_block)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
       "        2    0.000    0.000    0.000    0.000 base.py:592(_reset_identity)\n",
       "        2    0.000    0.000    0.000    0.000 managers.py:331(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
       "        6    0.000    0.000    0.000    0.000 common.py:211(classes_and_not_datetimelike)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x00007FF856FD79A0}\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method _warnings._filters_mutated}\n",
       "        6    0.000    0.000    0.000    0.000 blocks.py:213(internal_values)\n",
       "        4    0.000    0.000    0.000    0.000 common.py:192(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
       "        2    0.000    0.000    0.000    0.000 fromnumeric.py:1108(_argmax_dispatcher)\n",
       "        4    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_scalar}\n",
       "        6    0.000    0.000    0.000    0.000 numeric.py:155(is_all_dates)\n",
       "        2    0.000    0.000    0.000    0.000 misc.py:184(_datacopied)\n",
       "        2    0.000    0.000    0.000    0.000 multiarray.py:1309(may_share_memory)\n",
       "        2    0.000    0.000    0.000    0.000 fromnumeric.py:3227(_mean_dispatcher)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
       "        2    0.000    0.000    0.000    0.000 numeric.py:249(inferred_type)\n",
       "        2    0.000    0.000    0.000    0.000 fromnumeric.py:2087(_sum_dispatcher)\n",
       "        1    0.000    0.000    0.000    0.000 blocks.py:335(shape)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        2    0.000    0.000    0.000    0.000 multiarray.py:635(result_type)\n",
       "        2    0.000    0.000    0.000    0.000 numeric.py:83(_validate_dtype)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun -s cumulative todoJunto.PCA_from_sklearn(df_nutrientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-07 s\n",
       "\n",
       "Total time: 0.0289618 s\n",
       "File: ./../..\\src\\pca\\todoJunto.py\n",
       "Function: PCA_from_sklearn at line 20\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    20                                           def PCA_from_sklearn(X):\n",
       "    21                                               \"\"\"\n",
       "    22                                               componentes_principales(X): Función que devuelve las componentes principales.\n",
       "    23                                               \n",
       "    24                                               Parámetros\n",
       "    25                                               ----------\n",
       "    26                                               n_components: número de componentes. \n",
       "    27                                               svd_solver: str {‘auto’, ‘full’, ‘arpack’, ‘randomized’}\n",
       "    28                                                   Se elige 'full', lo que significa que se ejecuta completamente SVD llamando al \n",
       "    29                                                   solucionador estándar LAPACK a través de scipy.linalg.svd y se seleccionan los componentes mediante postprocessing.\n",
       "    30                                                   \n",
       "    31                                               Atributos\n",
       "    32                                               ---------\n",
       "    33                                               varianza_explicada: porcentaje de varianza explicada por cada componente.\n",
       "    34                                               valores_singulares: valores singulares correspondientes a cada componente.\n",
       "    35                                               pca.components_: ejes principales que representan las direcciones de máxima varianza en los datos.\n",
       "    36                                               eigenvalues: son los valores propios utilizando la matriz de covarianza.\n",
       "    37                                               \n",
       "    38                                               Método\n",
       "    39                                               ---------\n",
       "    40                                               fit_transform: ajusta el modelo a los datos y aplica la reducción de dimensionalidad en los datos.\n",
       "    41                                               \"\"\"\n",
       "    42         1       3918.0   3918.0      1.4      X = pd.DataFrame(X)\n",
       "    43         1         36.0     36.0      0.0      n_components = len(X.columns)\n",
       "    44         1         65.0     65.0      0.0      pca_1 = PCA(n_components, svd_solver='full')\n",
       "    45         1     193991.0 193991.0     67.0      componentesprincipales_1 = pca_1.fit_transform(X)\n",
       "    46         1         25.0     25.0      0.0      pca_1.components_\n",
       "    47         1          7.0      7.0      0.0      var_exp = pca_1.explained_variance_ratio_\n",
       "    48                                               \n",
       "    49                                               ##Se obtiene el número de componentes a través de la varianza explicada acumulada de los componentes, la cual debe sumar 60%.\n",
       "    50         1         76.0     76.0      0.0      var_acumulada = var_exp.cumsum()\n",
       "    51         1         50.0     50.0      0.0      conteo = (var_acumulada)  <  0.8\n",
       "    52         1        141.0    141.0      0.0      n_componentes = conteo.sum() + 1\n",
       "    53         1         70.0     70.0      0.0      pca = PCA(n_componentes, svd_solver='full')\n",
       "    54         1      91154.0  91154.0     31.5      componentesprincipales = pca.fit_transform(X)\n",
       "    55         1         47.0     47.0      0.0      pca.components_\n",
       "    56         1          8.0      8.0      0.0      varianza_explicada = pca.explained_variance_ratio_\n",
       "    57         1          8.0      8.0      0.0      eigenvalues = pca.explained_variance_\n",
       "    58         1          8.0      8.0      0.0      val_sing = pca.singular_values_\n",
       "    59                                               \n",
       "    60         1         14.0     14.0      0.0      return pca, varianza_explicada, componentesprincipales, val_sing, pca.components_, eigenvalues"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f todoJunto.PCA_from_sklearn todoJunto.PCA_from_sklearn(df_nutrientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es notorio que las transformaciones que ejecuta PCA al llamar a decomp_svd.py y ejecutar las transformaciones de la función fit_transform() son los elementos que consumen más tiempo. Al tratarse de una función propia de sklearn, no existe demasiado margen para recortar tiempos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA_from_SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         60 function calls in 0.007 seconds\n",
       "\n",
       "   Ordered by: cumulative time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.000    0.000    0.007    0.007 {built-in method builtins.exec}\n",
       "        1    0.001    0.001    0.007    0.007 <string>:1(<module>)\n",
       "        1    0.001    0.001    0.006    0.006 todoJunto.py:63(PCA_from_SVD)\n",
       "        3    0.000    0.000    0.003    0.001 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
       "        1    0.000    0.000    0.003    0.003 <__array_function__ internals>:2(svd)\n",
       "        1    0.003    0.003    0.003    0.003 linalg.py:1458(svd)\n",
       "        3    0.001    0.000    0.001    0.000 {built-in method numpy.array}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 _methods.py:134(_mean)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(sum)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2092(sum)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:73(_wrapreduction)\n",
       "        5    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
       "        1    0.000    0.000    0.000    0.000 _methods.py:50(_count_reduce_items)\n",
       "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(transpose)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 _methods.py:36(_sum)\n",
       "        1    0.000    0.000    0.000    0.000 linalg.py:144(_commonType)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:604(transpose)\n",
       "        1    0.000    0.000    0.000    0.000 linalg.py:116(_makearray)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 linalg.py:134(_realType)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:55(_wrapfunc)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 linalg.py:111(get_linalg_error_extobj)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "        1    0.000    0.000    0.000    0.000 _asarray.py:88(asanyarray)\n",
       "        1    0.000    0.000    0.000    0.000 linalg.py:203(_assert_stacked_2d)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:74(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 _asarray.py:16(asarray)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
       "        2    0.000    0.000    0.000    0.000 linalg.py:121(isComplexType)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {method '__array_prepare__' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:600(_transpose_dispatcher)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2087(_sum_dispatcher)\n",
       "        1    0.000    0.000    0.000    0.000 linalg.py:1454(_svd_dispatcher)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun -s cumulative todoJunto.PCA_from_SVD(df_nutrientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-07 s\n",
       "\n",
       "Total time: 0.0119768 s\n",
       "File: ./../..\\src\\pca\\todoJunto.py\n",
       "Function: PCA_from_SVD at line 63\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    63                                           def PCA_from_SVD(A):\n",
       "    64                                               \"\"\"\n",
       "    65                                               Función para PCA a partir de la SVD de numpy \n",
       "    66                                               params: A\t\t\tmatriz de datos\n",
       "    67                                                       num_componentes \tnúmero de componentes deseados\n",
       "    68                                           \n",
       "    69                                               return: valores_singulares\tLos valores singulares de la descomposición SVD\n",
       "    70                                           \t    componentes\t\tLos coeficientes para calcular los componentes principales\n",
       "    71                                           \t    Z\t\t\tLos datos transformados (componentes principales)\n",
       "    72                                           \t    varianza_explicada\tLa varianza explicada por cada componente principal\n",
       "    73                                               \"\"\"\n",
       "    74                                               \n",
       "    75                                               # Centrar los datos\n",
       "    76         1       8844.0   8844.0      7.4      A = np.array(A) # convertir los datos a un numpy array por si vienen de un DataFrame\n",
       "    77         1      11183.0  11183.0      9.3      A_centered = A - A.mean(axis=0)\n",
       "    78                                               \n",
       "    79                                               # Calcular SVD\n",
       "    80         1      76841.0  76841.0     64.2      U, S, Vt = np.linalg.svd(A_centered, full_matrices=False)\n",
       "    81                                               \n",
       "    82                                               # Los valores singulares\n",
       "    83         1         16.0     16.0      0.0      valores_singulares = S\n",
       "    84                                               \n",
       "    85                                               # Los componentes (coeficientes)\n",
       "    86         1          7.0      7.0      0.0      componentes = ((Vt))\n",
       "    87                                               \n",
       "    88                                               # Los datos transformados (componentes principales)\n",
       "    89         1      21749.0  21749.0     18.2      Z = A_centered@np.transpose(Vt)\n",
       "    90                                               \n",
       "    91                                               # La varianza explicada\n",
       "    92         1        812.0    812.0      0.7      varianza_explicada = S**2/np.sum(S**2)\n",
       "    93                                               \n",
       "    94                                               # Calcula número de componentes de manera automatica de acuerdo a la variana explicada\n",
       "    95                                               # Threshold de 60%\n",
       "    96         1         22.0     22.0      0.0      n = A.shape[1] #numero de columnas\n",
       "    97         1         60.0     60.0      0.1      varianza_acumulada = varianza_explicada.cumsum()\n",
       "    98         1         51.0     51.0      0.0      conteo = (varianza_acumulada)  <  0.8\n",
       "    99         1        134.0    134.0      0.1      num_componentes = conteo.sum() + 1\n",
       "   100                                               \n",
       "   101                                               # regresar 4 objetos\n",
       "   102         1         49.0     49.0      0.0      return valores_singulares[:num_componentes], componentes[:num_componentes], Z[:,:num_componentes], varianza_explicada[:num_componentes]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f todoJunto.PCA_from_SVD todoJunto.PCA_from_SVD(df_nutrientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El llamado a linalg.svd es el elemento que consume casi la mitad del tiempo. Al tratarse de una función propia de numpy, no existe demasiado margen para recortar tiempos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA_from_QR_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         769281 function calls (769280 primitive calls) in 1.527 seconds\n",
       "\n",
       "   Ordered by: cumulative time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.000    0.000    1.527    1.527 {built-in method builtins.exec}\n",
       "        1    0.002    0.002    1.527    1.527 <string>:1(<module>)\n",
       "        1    0.003    0.003    1.524    1.524 todoJunto.py:147(PCA_from_QR_vf)\n",
       "        1    0.016    0.016    1.518    1.518 algoritmo_QR.py:10(eigenvectores_eigenvalores_QR_vf)\n",
       "      451    0.215    0.000    1.488    0.003 funciones_examen_QR.py:168(matriz_Q_R)\n",
       "    20746    0.267    0.000    0.736    0.000 funciones_examen_QR.py:208(Q_j)\n",
       "      451    0.156    0.000    0.531    0.001 funciones_examen_QR.py:112(matriz_auxiliar_Arv)\n",
       "83889/83888    0.133    0.000    0.424    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
       "    31119    0.023    0.000    0.292    0.000 <__array_function__ internals>:2(outer)\n",
       "    10373    0.138    0.000    0.249    0.000 funciones_examen_QR.py:69(house)\n",
       "    31119    0.194    0.000    0.245    0.000 numeric.py:824(outer)\n",
       "    41943    0.090    0.000    0.154    0.000 twodim_base.py:154(eye)\n",
       "    31120    0.024    0.000    0.133    0.000 <__array_function__ internals>:2(concatenate)\n",
       "    41944    0.064    0.000    0.064    0.000 {built-in method numpy.zeros}\n",
       "    20746    0.014    0.000    0.059    0.000 <__array_function__ internals>:2(transpose)\n",
       "    31120    0.046    0.000    0.046    0.000 {method 'dot' of 'numpy.ndarray' objects}\n",
       "    62238    0.016    0.000    0.033    0.000 _asarray.py:16(asarray)\n",
       "    20746    0.010    0.000    0.032    0.000 fromnumeric.py:604(transpose)\n",
       "    20746    0.012    0.000    0.023    0.000 fromnumeric.py:55(_wrapfunc)\n",
       "    62693    0.019    0.000    0.019    0.000 {built-in method numpy.array}\n",
       "    62238    0.017    0.000    0.017    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
       "    29154    0.017    0.000    0.017    0.000 {built-in method builtins.pow}\n",
       "      451    0.000    0.000    0.009    0.000 <__array_function__ internals>:2(sum)\n",
       "      451    0.001    0.000    0.008    0.000 fromnumeric.py:2092(sum)\n",
       "    21648    0.006    0.000    0.006    0.000 {built-in method builtins.getattr}\n",
       "      451    0.002    0.000    0.006    0.000 fromnumeric.py:73(_wrapreduction)\n",
       "      450    0.000    0.000    0.006    0.000 <__array_function__ internals>:2(trace)\n",
       "    31120    0.005    0.000    0.005    0.000 multiarray.py:145(concatenate)\n",
       "    31119    0.005    0.000    0.005    0.000 numeric.py:820(_outer_dispatcher)\n",
       "      902    0.002    0.000    0.005    0.000 copy.py:66(copy)\n",
       "    20746    0.005    0.000    0.005    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
       "      450    0.001    0.000    0.005    0.000 fromnumeric.py:1625(trace)\n",
       "    21650    0.005    0.000    0.005    0.000 {built-in method builtins.isinstance}\n",
       "      453    0.004    0.000    0.004    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "      450    0.003    0.000    0.003    0.000 {method 'trace' of 'numpy.ndarray' objects}\n",
       "    20746    0.003    0.000    0.003    0.000 fromnumeric.py:600(_transpose_dispatcher)\n",
       "    10373    0.002    0.000    0.002    0.000 {built-in method builtins.len}\n",
       "      902    0.002    0.000    0.002    0.000 {method '__copy__' of 'numpy.ndarray' objects}\n",
       "      451    0.001    0.000    0.001    0.000 fromnumeric.py:74(<dictcomp>)\n",
       "      453    0.000    0.000    0.000    0.000 _asarray.py:88(asanyarray)\n",
       "      902    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(mean)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:3231(mean)\n",
       "        1    0.000    0.000    0.000    0.000 _methods.py:134(_mean)\n",
       "      904    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
       "      451    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
       "      451    0.000    0.000    0.000    0.000 fromnumeric.py:2087(_sum_dispatcher)\n",
       "      450    0.000    0.000    0.000    0.000 fromnumeric.py:1620(_trace_dispatcher)\n",
       "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(append)\n",
       "        1    0.000    0.000    0.000    0.000 function_base.py:4640(append)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(diagonal)\n",
       "        1    0.000    0.000    0.000    0.000 _methods.py:36(_sum)\n",
       "        1    0.000    0.000    0.000    0.000 _methods.py:50(_count_reduce_items)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:1490(diagonal)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'diagonal' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 function_base.py:4636(_append_dispatcher)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:3227(_mean_dispatcher)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:1486(_diagonal_dispatcher)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun -s cumulative todoJunto.PCA_from_QR_vf(df_nutrientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-07 s\n",
       "\n",
       "Total time: 1.6004 s\n",
       "File: ./../..\\src\\pca\\todoJunto.py\n",
       "Function: PCA_from_QR_vf at line 147\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   147                                           def PCA_from_QR_vf(data,niter = 450):\n",
       "   148                                               \"\"\"\n",
       "   149                                               Función para PCA a partir de los eigenvectores  \n",
       "   150                                               params: data:\t\t\tmatriz de datos\n",
       "   151                                                       niter:      número de iteraciones máximas    \n",
       "   152                                               \n",
       "   153                                               \n",
       "   154                                               return:     componentes\t\tLos coeficientes para calcular los componentes principales (eigenvectores de la matriz de covarianzas)\n",
       "   155                                                           Z\t\t\tLos datos transformados (componentes principales)\n",
       "   156                                                           varianza_explicada\tLa varianza explicada por cada componente principal\n",
       "   157                                               \n",
       "   158                                               Depende de la función: eigenvectores_QR\n",
       "   159                                               \"\"\"\n",
       "   160                                               \n",
       "   161                                               # convertir a array\n",
       "   162         1       8261.0   8261.0      0.1      A = np.array(data)\n",
       "   163                                               \n",
       "   164                                               # Centrar los datos\n",
       "   165         1       2276.0   2276.0      0.0      mean_vec = np.mean(A, axis=0)\n",
       "   166         1       7180.0   7180.0      0.0      datos_centrados = (A - mean_vec)\n",
       "   167                                           \n",
       "   168                                               # Matriz de Covarianzas\n",
       "   169                                               #C = (datos_centrados.T@datos_centrados)/(datos_centrados.shape[0]-1)\n",
       "   170         1      22848.0  22848.0      0.1      C = (A - mean_vec).T.dot((A - mean_vec)) / (A.shape[0]-1)\n",
       "   171                                               \n",
       "   172                                               # Calcular algoritmo QR\n",
       "   173         1   15958205.0 15958205.0     99.7      E, Q = eigenvectores_eigenvalores_QR_vf(C,niter)\n",
       "   174                                               \n",
       "   175                                               \n",
       "   176                                               # Los componentes (coeficientes)\n",
       "   177         1         22.0     22.0      0.0      componentes = Q.T\n",
       "   178                                               \n",
       "   179                                               # Los datos transformados (componentes principales)\n",
       "   180                                               # Aquí marcaba error al filtrar porque no se reconocia a Z como numpy array\n",
       "   181         1       4362.0   4362.0      0.0      Z = datos_centrados@Q\n",
       "   182                                               \n",
       "   183                                               # La varianza explicada\n",
       "   184         1        350.0    350.0      0.0      varianza_explicada = E/np.sum(E)\n",
       "   185                                               \n",
       "   186                                               # Calcula número de componentes de manera automatica de acuerdo a la variana explicada\n",
       "   187                                               # Threshold de 60%\n",
       "   188         1         16.0     16.0      0.0      n = data.shape[1] #numero de columnas\n",
       "   189         1         54.0     54.0      0.0      varianza_acumulada = varianza_explicada.cumsum()\n",
       "   190         1        163.0    163.0      0.0      conteo = (varianza_acumulada)  <  0.8\n",
       "   191         1        220.0    220.0      0.0      num_componentes = conteo.sum() + 1\n",
       "   192                                               \n",
       "   193                                               # regresar 4 objetos\n",
       "   194         1         36.0     36.0      0.0      return E[:num_componentes], componentes[:num_componentes], Z[:,:num_componentes], varianza_explicada[:num_componentes] #, varianza_acumulada, num_componentes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f todoJunto.PCA_from_QR_vf todoJunto.PCA_from_QR_vf(df_nutrientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayor parte del tiempo lo realiza la función eigenvectores_eigenvalores_QR_vf la cual tuvo como elementos con más utilización de tiempo a los llamados a las siguientes funciones\n",
    "* **matriz_Q_R**\n",
    "* **matriz_auxiliar_Arv**\n",
    "\n",
    "por lo que se realiza un lprun de las mismas. Se comprueba que el cómputo más intensivo se encuentra en las iteraciones ocurridas en estas funciones, en específico en la multiplicación que genera las componentes r de la matriz R dentro del algoritmo matriz_auxiliar_Arv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-07 s\n",
       "\n",
       "Total time: 0.0045808 s\n",
       "File: ./../..\\src\\pca\\funciones_examen_QR.py\n",
       "Function: matriz_Q_R at line 168\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   168                                           def matriz_Q_R(A,ind_singular=False):\n",
       "   169                                               \"\"\"\n",
       "   170                                               Función que devuelve la matriz R y Q de la factorización QR de una matriz A\n",
       "   171                                               \n",
       "   172                                               params: A    Matriz (mxn)\n",
       "   173                                               return: Q    Matriz Q (mxm) de la factorización A=QR\n",
       "   174                                                       R    Matriz Q (mxm) de la factorización A=QR\n",
       "   175                                               \"\"\"\n",
       "   176                                               \n",
       "   177                                               #Se checa que los parámetros sean congruentes con la funcionalidad\n",
       "   178         1         20.0     20.0      0.0      if type(A) is not np.ndarray:\n",
       "   179                                                   sys.exit('A debe ser de tipo numpy.ndarray')\n",
       "   180         1         22.0     22.0      0.0      elif A.shape[0]<A.shape[1]:\n",
       "   181                                                   sys.exit('El numero de renglones de A tiene que ser igual o mayor al no. de columnas')\n",
       "   182                                               \n",
       "   183                                               #si ind_singular es True, entonces no se devuelve la matriz R ni Q\n",
       "   184                                               #esto se plantea así como un sistema de contról, para evitar comprobar que las matrices\n",
       "   185                                               #sean singulares (y no tengan una única solución) mediante el determinante, que\n",
       "   186                                               #involucra un alto costo computacional para matrices de tamaño representativo\n",
       "   187         1      18871.0  18871.0     41.2      Arv=matriz_auxiliar_Arv(A,ind_singular)\n",
       "   188                                           \n",
       "   189         1         17.0     17.0      0.0      if Arv is None:\n",
       "   190                                                   return None, None\n",
       "   191                                               else:\n",
       "   192         1         13.0     13.0      0.0          m,n=A.shape\n",
       "   193         1        151.0    151.0      0.3          Q=np.eye(m)\n",
       "   194         1         99.0     99.0      0.2          R=copy.copy(A)\n",
       "   195        24        222.0      9.2      0.5          for j in range(n):\n",
       "   196        23      12883.0    560.1     28.1              Qj=Q_j(Arv,j+1)\n",
       "   197        23       1768.0     76.9      3.9              Q=Q@Qj\n",
       "   198        23      11737.0    510.3     25.6              R=Q_j(Arv,j+1)@R\n",
       "   199         1          5.0      5.0      0.0          return Q,R"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # convertir a array\n",
    "    A = np.array(df_nutrientes)\n",
    "    \n",
    "    # Centrar los datos\n",
    "    mean_vec = np.mean(A, axis=0)\n",
    "    datos_centrados = (A - mean_vec)\n",
    "\n",
    "    # Matriz de Covarianzas\n",
    "    C = (A - mean_vec).T.dot((A - mean_vec)) / (A.shape[0]-1)\n",
    "    \n",
    "    n, p = C.shape\n",
    "    columnas = n - p\n",
    "    ceros = np.zeros((n,columnas))\n",
    "    \n",
    "    # Matriz inicial\n",
    "    A0 = np.append(C, ceros, axis = 1)\n",
    "\n",
    "\n",
    "from src.pca.funciones_examen_QR import matriz_Q_R\n",
    "%lprun -f matriz_Q_R matriz_Q_R(A0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-07 s\n",
       "\n",
       "Total time: 0.0013027 s\n",
       "File: ./../..\\src\\pca\\funciones_examen_QR.py\n",
       "Function: matriz_auxiliar_Arv at line 112\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   112                                           def matriz_auxiliar_Arv(A,ind_singular=False):\n",
       "   113                                               \"\"\"\n",
       "   114                                               Función que genera una matriz que contiene los elementos r distintos de cero de la \n",
       "   115                                               matriz R y las entradas de los vectores householder v (excepto la primera), con los \n",
       "   116                                               cuales se puede calcular la matriz Q. Ambas matrices componentes de la factorización QR\n",
       "   117                                               \n",
       "   118                                               params: A      Matriz (mxn) de la que se desea obtner factorización QR\n",
       "   119                                                       \n",
       "   120                                               return: Arv    Matriz (mxn) que incluye las componentes distintas de cero de la matriz R \n",
       "   121                                                              y los vectores householder con los que se puede obtener la matriz Q, y \n",
       "   122                                                              con ello la factorización QR\n",
       "   123                                               \"\"\"\n",
       "   124                                               \n",
       "   125                                               #Se checa que los parámetros sean congruentes con la funcionalidad\n",
       "   126         1         26.0     26.0      0.2      if type(A) is not np.ndarray:\n",
       "   127                                                   sys.exit('A debe ser de tipo numpy.ndarray')\n",
       "   128         1         14.0     14.0      0.1      m,n=A.shape\n",
       "   129         1          8.0      8.0      0.1      if m<n:\n",
       "   130                                                   sys.exit('EL numero de renglones de A debe ser mayor o igual al no. de columnas')\n",
       "   131                                               \n",
       "   132                                               #m contiene el numero de renglones y n el de columnas\n",
       "   133         1          7.0      7.0      0.1      m,n=A.shape\n",
       "   134                                               #se crea una matriz con los valores de A\n",
       "   135         1        101.0    101.0      0.8      Arv=copy.copy(A)\n",
       "   136                                               \n",
       "   137                                               #si no se pide que se cheque que la matriz es singular, se obtendrá la matriz R\n",
       "   138                                               #aún si esta es singula (que exista algún elemento de la diagonal que sea 0\n",
       "   139         1          8.0      8.0      0.1      if ind_singular==False:\n",
       "   140        24        158.0      6.6      1.2          for j in range(n):\n",
       "   141        23       6387.0    277.7     49.0              beta, v=house(Arv[j:m,j])\n",
       "   142                                                       #Con esta multiplicación se van generando las componentes r de la matriz R\n",
       "   143        23       5832.0    253.6     44.8              Arv[j:m,j:n]=Arv[j:m,j:n]-beta*np.outer(v,(np.transpose(v)@Arv[j:m,j:n]))\n",
       "   144                                                       #se guarda en cada columnas los valores de v d, excepto la primer componente (que vale 1)\n",
       "   145        23        480.0     20.9      3.7              Arv[(j+1):m,j]=v[1:(m-j)]\n",
       "   146                                               else:\n",
       "   147                                                   for j in range(n):\n",
       "   148                                                       beta, v=house(Arv[j:m,j])\n",
       "   149                                                       Arv[j:m,j:n]=Arv[j:m,j:n]-beta*np.outer(v,(np.transpose(v)@Arv[j:m,j:n]))\n",
       "   150                                                       #Si se detecta que alguna entrada de la diagonal (elemento de la maatriz R)\n",
       "   151                                                       #es cero, se devuelve un valor nulo (None), y para el cálculo de Arv\n",
       "   152                                                       #No se condiciona sobre si alcanza el valor de cero, sino más bien\n",
       "   153                                                       #si está lo suficientemente cercano, pues dados los errores por redondeo\n",
       "   154                                                       #no siempre se alcanza el cero aunque teóricamente sí lo sea, pero el\n",
       "   155                                                       #algoritmos arroja un valor cercano a cero\n",
       "   156                                                       if Arv[j,j]<10**(-14) and Arv[j,j]>-10**(-14):\n",
       "   157                                                           return None\n",
       "   158                                                       Arv[(j+1):m,j]=v[1:(m-j)]\n",
       "   159         1          6.0      6.0      0.0      return Arv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # convertir a array\n",
    "    A = np.array(df_nutrientes)\n",
    "    \n",
    "    # Centrar los datos\n",
    "    mean_vec = np.mean(A, axis=0)\n",
    "    datos_centrados = (A - mean_vec)\n",
    "\n",
    "    # Matriz de Covarianzas\n",
    "    C = (A - mean_vec).T.dot((A - mean_vec)) / (A.shape[0]-1)\n",
    "    \n",
    "    n, p = C.shape\n",
    "    columnas = n - p\n",
    "    ceros = np.zeros((n,columnas))\n",
    "    \n",
    "    # Matriz inicial\n",
    "    A0 = np.append(C, ceros, axis = 1)\n",
    "\n",
    "\n",
    "from src.pca.funciones_examen_QR import matriz_auxiliar_Arv\n",
    "%lprun -f matriz_auxiliar_Arv matriz_auxiliar_Arv(A0,ind_singular=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por la naturaleza iterativa del algoritmo así como del tamaño de la matriz introducida, se justifica que el algoritmo es computacionalmente más intensivo que los demás que se encuentran mucho más optimizados dentro de las librerías Numpy y ScyKit Learn de Python. **Se sugiere paralelizar esta parte del código o la función en su totalidad para obtener mejores resultados.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA_from_potencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         736545 function calls (690545 primitive calls) in 0.585 seconds\n",
       "\n",
       "   Ordered by: cumulative time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.000    0.000    0.585    0.585 {built-in method builtins.exec}\n",
       "        1    0.000    0.000    0.585    0.585 <string>:1(<module>)\n",
       "        1    0.002    0.002    0.585    0.585 todoJunto.py:196(PCA_from_potencia)\n",
       "        1    0.001    0.001    0.580    0.580 metodo_potencia_deflation.py:38(power_deflation)\n",
       "       23    0.125    0.005    0.579    0.025 metodo_potencia_deflation.py:5(power_iteration)\n",
       "138049/92049    0.132    0.000    0.394    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
       "    46000    0.026    0.000    0.367    0.000 <__array_function__ internals>:2(norm)\n",
       "    46000    0.161    0.000    0.310    0.000 linalg.py:2316(norm)\n",
       "    92001    0.050    0.000    0.167    0.000 <__array_function__ internals>:2(dot)\n",
       "    46092    0.024    0.000    0.024    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
       "    46092    0.012    0.000    0.023    0.000 _asarray.py:16(asarray)\n",
       "    46000    0.012    0.000    0.016    0.000 linalg.py:121(isComplexType)\n",
       "    46094    0.013    0.000    0.013    0.000 {built-in method numpy.array}\n",
       "    92002    0.011    0.000    0.011    0.000 {built-in method builtins.issubclass}\n",
       "    92001    0.010    0.000    0.010    0.000 multiarray.py:707(dot)\n",
       "    46000    0.006    0.000    0.006    0.000 linalg.py:2312(_norm_dispatcher)\n",
       "       46    0.000    0.000    0.001    0.000 <__array_function__ internals>:2(outer)\n",
       "        1    0.000    0.000    0.001    0.001 <__array_function__ internals>:2(mean)\n",
       "        1    0.000    0.000    0.001    0.001 fromnumeric.py:3231(mean)\n",
       "        1    0.000    0.000    0.001    0.001 _methods.py:134(_mean)\n",
       "       46    0.000    0.000    0.001    0.000 numeric.py:824(outer)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "       23    0.000    0.000    0.000    0.000 {method 'rand' of 'numpy.random.mtrand.RandomState' objects}\n",
       "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(sum)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2092(sum)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:73(_wrapreduction)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method numpy.zeros}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 _methods.py:36(_sum)\n",
       "       46    0.000    0.000    0.000    0.000 numeric.py:820(_outer_dispatcher)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 _methods.py:50(_count_reduce_items)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 _asarray.py:88(asanyarray)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:74(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:3227(_mean_dispatcher)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2087(_sum_dispatcher)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun -s cumulative todoJunto.PCA_from_potencia(df_nutrientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-07 s\n",
       "\n",
       "Total time: 0.838369 s\n",
       "File: ./../..\\src\\pca\\todoJunto.py\n",
       "Function: PCA_from_potencia at line 196\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   196                                           def PCA_from_potencia(X):\n",
       "   197                                               \"\"\"\n",
       "   198                                               Función que calcula PCA a partir del método de la potencia y deflation de Hotteling  \n",
       "   199                                               params: A:\t\t\tmatriz de datos\n",
       "   200                                               \n",
       "   201                                               \n",
       "   202                                               return:     eigenvalues\t\tNumpy array con los eigenvectores de A\n",
       "   203                                                           eigenvectors\tNumpy array con los correspondientes eigenvectores de A \n",
       "   204                                               \n",
       "   205                                               \"\"\"\n",
       "   206                                               \n",
       "   207         1         15.0     15.0      0.0      prop = 0 # Proporción de varianza explicada\n",
       "   208         1          8.0      8.0      0.0      comp = 1 \n",
       "   209         1          5.0      5.0      0.0      cur_var = 0\n",
       "   210         1        110.0    110.0      0.0      comp_vecs = np.zeros([X.shape[1], X.shape[1]])\n",
       "   211                                               \n",
       "   212                                               # convertir a array\n",
       "   213         1       8882.0   8882.0      0.1      A = np.array(X)\n",
       "   214                                               \n",
       "   215                                               # Centrar los datos\n",
       "   216         1       2725.0   2725.0      0.0      mean_vec = np.mean(A, axis=0)\n",
       "   217         1       7656.0   7656.0      0.1      datos_centrados = (A - mean_vec)\n",
       "   218                                               \n",
       "   219                                               #Calculamos la matriz de covarianzas\n",
       "   220         1       2874.0   2874.0      0.0      cov = np.dot(X.T, X)/X.shape[0]\n",
       "   221                                               \n",
       "   222                                               #Aplicamos el método de la potencia\n",
       "   223         1    8356424.0 8356424.0     99.7      evalues_pow, evectors_pow = power_deflation(cov,2000)\n",
       "   224                                               \n",
       "   225                                               # La varianza explicada\n",
       "   226         1        303.0    303.0      0.0      varianza_explicada = evalues_pow/np.sum(evalues_pow)\n",
       "   227                                               \n",
       "   228                                               # Los datos transformados (componentes principales)\n",
       "   229         1       4295.0   4295.0      0.1      Z = datos_centrados@evectors_pow\n",
       "   230                                               \n",
       "   231                                               \n",
       "   232                                               # Calcula número de componentes de manera automatica de acuerdo a la variana explicada\n",
       "   233                                               # Threshold de 80%\n",
       "   234         1         26.0     26.0      0.0      n = X.shape[1] #numero de columnas\n",
       "   235         1         82.0     82.0      0.0      varianza_acumulada = varianza_explicada.cumsum()\n",
       "   236         1         67.0     67.0      0.0      conteo = (varianza_acumulada)  <  0.8\n",
       "   237         1        187.0    187.0      0.0      num_componentes = conteo.sum() + 1\n",
       "   238                                               \n",
       "   239         1         35.0     35.0      0.0      return evalues_pow[:num_componentes], evectors_pow[:num_componentes], Z[:,:num_componentes], varianza_explicada[:num_componentes]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f todoJunto.PCA_from_potencia todoJunto.PCA_from_potencia(df_nutrientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente la parte más intensiva en tiempo y cómputo corresponde a la **iteración** que realiza el **método de deflation:** el cual tiene el número de iteraciones en hard-code con 2000 de las mismas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-07 s\n",
       "\n",
       "Total time: 0.861516 s\n",
       "File: ./../..\\src\\pca\\metodo_potencia_deflation.py\n",
       "Function: power_deflation at line 38\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    38                                           def power_deflation(A,iter):\n",
       "    39                                               \"\"\"\n",
       "    40                                               Función que itera el método de la potencia, mediante deflation de Hotteling  \n",
       "    41                                               params: A:\t\t\tmatriz (en nuestro caso, la de covarianzas)\n",
       "    42                                                       iter:      número de iteraciones a realizar    \n",
       "    43                                               \n",
       "    44                                               \n",
       "    45                                               return:     eigenvalues\t\tNumpy array con los eigenvectores de A\n",
       "    46                                                           eigenvectors\tNumpy array con los correspondientes eigenvectores de A \n",
       "    47                                           \n",
       "    48                                               \"\"\"\n",
       "    49                                               #numero de columnas\n",
       "    50         1         21.0     21.0      0.0      n = A.shape[1]\n",
       "    51                                               # Inicializamos arrays de ceros\n",
       "    52         1         49.0     49.0      0.0      eigenvalues = np.zeros(n)\n",
       "    53         1         26.0     26.0      0.0      eigenvectors = np.zeros((n,n))\n",
       "    54                                               #Hago una copia de la matriz original\n",
       "    55         1         38.0     38.0      0.0      A_def = A.copy()\n",
       "    56                                               #Iteramos tantas veces como columnas de la matriz\n",
       "    57        24        161.0      6.7      0.0      for i in range(n):\n",
       "    58                                                   #Aplicamos el método de la potencia\n",
       "    59        23    8601316.0 373970.3     99.8          m_def,b_def = power_iteration(A_def,iter)\n",
       "    60                                                   #Actualizamos los arrays de eigen valores y vectores\n",
       "    61        23        927.0     40.3      0.0          eigenvalues[i] = m_def\n",
       "    62        23        777.0     33.8      0.0          eigenvectors[:,i]= b_def\n",
       "    63                                                   # Matriz actualizada\n",
       "    64        23      11846.0    515.0      0.1          A_def = A_def - np.outer(b_def,b_def)@A_def@np.outer(b_def,b_def)\n",
       "    65         1          4.0      4.0      0.0      return eigenvalues, eigenvectors"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Calculamos la matriz de covarianzas\n",
    "cov = np.dot(df_nutrientes.T, df_nutrientes)/df_nutrientes.shape[0]\n",
    "\n",
    "from src.pca.metodo_potencia_deflation import power_deflation\n",
    "%lprun -f power_deflation power_deflation(cov,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-07 s\n",
       "\n",
       "Total time: 0.0503572 s\n",
       "File: ./../..\\src\\pca\\metodo_potencia_deflation.py\n",
       "Function: power_iteration at line 5\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     5                                           def power_iteration(A, num_simulations: int):\n",
       "     6                                               \"\"\"\n",
       "     7                                               Función que implementa el método de la potencia  \n",
       "     8                                               params: A:\t\t\tmatriz (en nuestro caso, la de covarianzas)\n",
       "     9                                                       num_simulations:      número de iteraciones a realizar    \n",
       "    10                                               \n",
       "    11                                               \n",
       "    12                                               return:     m_k\t\tEigenvector dominante\n",
       "    13                                                           b_k\t\tEigenvector corresondiente\n",
       "    14                                           \n",
       "    15                                               \"\"\"\n",
       "    16                                               # Ideally choose a random vector\n",
       "    17                                               # To decrease the chance that our vector\n",
       "    18                                               # Is orthogonal to the eigenvector\n",
       "    19         1        158.0    158.0      0.0      b_k = np.random.rand(A.shape[1])\n",
       "    20                                           \n",
       "    21      2001       9921.0      5.0      2.0      for _ in range(num_simulations):\n",
       "    22                                                   # calculate the matrix-by-vector product Ab\n",
       "    23      2000      91743.0     45.9     18.2          b_k1 = np.dot(A, b_k)\n",
       "    24                                           \n",
       "    25                                                   # calculate the norm\n",
       "    26      2000     345342.0    172.7     68.6          b_k1_norm = np.linalg.norm(b_k1)\n",
       "    27                                           \n",
       "    28                                                   # re normalize the vector\n",
       "    29      2000      56226.0     28.1     11.2          b_k = b_k1 / b_k1_norm\n",
       "    30                                               \n",
       "    31                                               #Obtenemos el eigenvalor correspondiente a b_k con el cociente de Rayleigh\n",
       "    32         1        177.0    177.0      0.0      m_k = (b_k.T@A@b_k)/(b_k.T@b_k)\n",
       "    33                                               \n",
       "    34                                               #Devolvemos el mayor eigenvalor y su correspondiente eigenvector\n",
       "    35         1          5.0      5.0      0.0      return m_k,b_k"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cov = np.dot(df_nutrientes.T, df_nutrientes)/df_nutrientes.shape[0]\n",
    "A_def = cov.copy()\n",
    "from src.pca.metodo_potencia_deflation import power_iteration\n",
    "%lprun -f power_iteration power_iteration(A_def,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que la normalización del Eigenvector correspondiente y sus respectivas iteraciones son las más intensivas en consumo de tiempo por lo que se sugiere **paralelizar** esa parte del código o bien la función en su totalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Uso de memoria RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 A través de %memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 166.97 MiB, increment: 48.18 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit -c todoJunto.PCA_from_sklearn(df_nutrientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 167.29 MiB, increment: 46.22 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit -c todoJunto.PCA_from_SVD(df_nutrientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 164.20 MiB, increment: 43.05 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit -c todoJunto.PCA_from_QR_vf(df_nutrientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 164.31 MiB, increment: 43.03 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit -c todoJunto.PCA_from_potencia(df_nutrientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados de Memit reflejan una utilización similar de la memoria en todas las funciones, siendo las que implementamos en el proyectos ligeramente mejores **(aproximadamente entre un 1%y 2% mejores)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 A través de heapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = hpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Partition of a set of 15 objects. Total size = 2193 bytes.\n",
       " Index  Count   %     Size   % Cumulative  % Kind (class / dict of class)\n",
       "     0      3  20     1376  63      1376  63 types.FrameType\n",
       "     1      3  20      200   9      1576  72 list\n",
       "     2      1   7      144   7      1720  78 types.CodeType\n",
       "     3      2  13      128   6      1848  84 tuple\n",
       "     4      1   7      112   5      1960  89 dict of _ast.Module\n",
       "     5      1   7       64   3      2024  92 types.MethodType\n",
       "     6      1   7       56   3      2080  95 _ast.Module\n",
       "     7      1   7       45   2      2125  97 bytes\n",
       "     8      1   7       40   2      2165  99 _thread.lock\n",
       "     9      1   7       28   1      2193 100 int"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.setrelheap()\n",
    "h = hp.heap()\n",
    "todoJunto.PCA_from_sklearn(df_nutrientes)\n",
    "h = hp.heap()\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Partition of a set of 18 objects. Total size = 2337 bytes.\n",
       " Index  Count   %     Size   % Cumulative  % Kind (class / dict of class)\n",
       "     0      3  17     1376  59      1376  59 types.FrameType\n",
       "     1      3  17      200   9      1576  67 list\n",
       "     2      1   6      144   6      1720  74 types.CodeType\n",
       "     3      2  11      128   5      1848  79 tuple\n",
       "     4      1   6      112   5      1960  84 dict of _ast.Module\n",
       "     5      3  17       92   4      2052  88 int\n",
       "     6      1   6       80   3      2132  91 builtins.weakref\n",
       "     7      1   6       64   3      2196  94 types.MethodType\n",
       "     8      1   6       56   2      2252  96 _ast.Module\n",
       "     9      1   6       45   2      2297  98 bytes\n",
       "    10      1   6       40   2      2337 100 _thread.lock"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp2 = hpy()\n",
    "hp2.setrelheap()\n",
    "h2 = hp2.heap()\n",
    "todoJunto.PCA_from_SVD(df_nutrientes)\n",
    "h2 = hp2.heap()\n",
    "h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Partition of a set of 18 objects. Total size = 2337 bytes.\n",
       " Index  Count   %     Size   % Cumulative  % Kind (class / dict of class)\n",
       "     0      3  17     1376  59      1376  59 types.FrameType\n",
       "     1      3  17      200   9      1576  67 list\n",
       "     2      1   6      144   6      1720  74 types.CodeType\n",
       "     3      2  11      128   5      1848  79 tuple\n",
       "     4      1   6      112   5      1960  84 dict of _ast.Module\n",
       "     5      3  17       92   4      2052  88 int\n",
       "     6      1   6       80   3      2132  91 builtins.weakref\n",
       "     7      1   6       64   3      2196  94 types.MethodType\n",
       "     8      1   6       56   2      2252  96 _ast.Module\n",
       "     9      1   6       45   2      2297  98 bytes\n",
       "    10      1   6       40   2      2337 100 _thread.lock"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp3 = hpy()\n",
    "hp3.setrelheap()\n",
    "h3 = hp3.heap()\n",
    "todoJunto.PCA_from_QR_vf(df_nutrientes)\n",
    "h3 = hp3.heap()\n",
    "h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Partition of a set of 303 objects. Total size = 39529 bytes.\n",
       " Index  Count   %     Size   % Cumulative  % Kind (class / dict of class)\n",
       "     0     22   7    11368  29     11368  29 types.FrameType\n",
       "     1     37  12     5279  13     16647  42 str\n",
       "     2     16   5     4224  11     20871  53 dict (no owner)\n",
       "     3     30  10     2424   6     23295  59 list\n",
       "     4     30  10     2000   5     25295  64 tuple\n",
       "     5      7   2     1288   3     26583  67 zmq.sugar.frame.Frame\n",
       "     6     10   3     1120   3     27703  70 dict of _ast.Name\n",
       "     7      8   3     1088   3     28791  73 function\n",
       "     8     10   3     1078   3     29869  76 bytes\n",
       "     9     17   6      816   2     30685  78 builtins.cell\n",
       "<31 more rows. Type e.g. '_.more' to view.>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp4 = hpy()\n",
    "hp4.setrelheap()\n",
    "h4 = hp.heap()\n",
    "todoJunto.PCA_from_potencia(df_nutrientes)\n",
    "h4 = hp.heap()\n",
    "h4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con heapy podemos darnos cuenta que heapy muestra resultados distintos a memit ya que es posible que exista un memory leak en la función del método de la potencia pues genera objetos de tipo frame que demandan más memoria, provenientes de las iteraciones que identificamos previamente en los ejercicios con el CPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
